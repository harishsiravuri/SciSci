{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import textfeatures as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"regression_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"regression_data_scibert.csv\")\n",
    "data['similarity_score'] = pd.to_numeric(data['semantic_similarity_aftercleaning'].str.replace('[','').str.replace(']',''))\n",
    "data = data[data['similarity_score'] != 999999999.0].copy()\n",
    "data['similarity_score'] = data['similarity_score'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite being impactful on a  variety of probl...</td>\n",
       "      <td>Summary:\\r\\n\\r\\nThe paper proposes a new regul...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>despite impactful  variety problem application...</td>\n",
       "      <td>summary   paper propose new regularizer wgans ...</td>\n",
       "      <td>[[0.84920603]]</td>\n",
       "      <td>20.219680</td>\n",
       "      <td>5.711538</td>\n",
       "      <td>174.666667</td>\n",
       "      <td>68</td>\n",
       "      <td>84.920603</td>\n",
       "      <td>161</td>\n",
       "      <td>1053</td>\n",
       "      <td>5.711538</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence-to-sequence (Seq2Seq) models with att...</td>\n",
       "      <td>The paper proposes a novel approach to integra...</td>\n",
       "      <td>[[0.89935327]]</td>\n",
       "      <td>sequence   sequence  seq2seq  model attention ...</td>\n",
       "      <td>paper propose novel approach integrate languag...</td>\n",
       "      <td>[[0.8828948]]</td>\n",
       "      <td>47.344262</td>\n",
       "      <td>5.768519</td>\n",
       "      <td>181.750000</td>\n",
       "      <td>51</td>\n",
       "      <td>88.289480</td>\n",
       "      <td>108</td>\n",
       "      <td>730</td>\n",
       "      <td>5.768519</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning policies for complex tasks that requi...</td>\n",
       "      <td>This paper introduces an iterative method to b...</td>\n",
       "      <td>[[0.86849725]]</td>\n",
       "      <td>learning policy complex task require multiple ...</td>\n",
       "      <td>paper introduce iterative method build hierarc...</td>\n",
       "      <td>[[0.79635]]</td>\n",
       "      <td>13.491979</td>\n",
       "      <td>5.764706</td>\n",
       "      <td>126.777778</td>\n",
       "      <td>85</td>\n",
       "      <td>79.635000</td>\n",
       "      <td>170</td>\n",
       "      <td>1149</td>\n",
       "      <td>5.764706</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent work has demonstrated that neural netwo...</td>\n",
       "      <td>- The authors investigate a minimax formulatio...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recent work demonstrate neural network vulnera...</td>\n",
       "      <td>author investigate minimax formulation deep n...</td>\n",
       "      <td>[[0.7955401]]</td>\n",
       "      <td>29.983389</td>\n",
       "      <td>5.782051</td>\n",
       "      <td>131.250000</td>\n",
       "      <td>76</td>\n",
       "      <td>79.554010</td>\n",
       "      <td>156</td>\n",
       "      <td>1057</td>\n",
       "      <td>5.782051</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In high dimensions, the performance of nearest...</td>\n",
       "      <td>\\r\\nThe context is indexing images with descri...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>high dimension  performance near neighbor algo...</td>\n",
       "      <td>context index image descriptor vector obtain ...</td>\n",
       "      <td>[[0.79431677]]</td>\n",
       "      <td>20.686636</td>\n",
       "      <td>6.309091</td>\n",
       "      <td>133.833333</td>\n",
       "      <td>55</td>\n",
       "      <td>79.431677</td>\n",
       "      <td>105</td>\n",
       "      <td>808</td>\n",
       "      <td>6.309091</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>\\emph{Truncated Backpropagation Through Time} ...</td>\n",
       "      <td>This paper proposes stochastic determination m...</td>\n",
       "      <td>[[0.84886956]]</td>\n",
       "      <td>emphtruncated backpropagation time   truncate ...</td>\n",
       "      <td>paper propose stochastic determination method ...</td>\n",
       "      <td>[[0.81641376]]</td>\n",
       "      <td>31.202941</td>\n",
       "      <td>6.913043</td>\n",
       "      <td>140.888889</td>\n",
       "      <td>81</td>\n",
       "      <td>81.641376</td>\n",
       "      <td>162</td>\n",
       "      <td>1276</td>\n",
       "      <td>6.913043</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>Actor-critic methods solve reinforcement learn...</td>\n",
       "      <td>The paper introduces a modified actor-critic a...</td>\n",
       "      <td>[[0.88001734]]</td>\n",
       "      <td>actor  critic method solve reinforcement learn...</td>\n",
       "      <td>paper introduce modify actor  critic algorithm...</td>\n",
       "      <td>[[0.79477006]]</td>\n",
       "      <td>13.005803</td>\n",
       "      <td>5.389937</td>\n",
       "      <td>112.111111</td>\n",
       "      <td>71</td>\n",
       "      <td>79.477006</td>\n",
       "      <td>159</td>\n",
       "      <td>1017</td>\n",
       "      <td>5.389937</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>The graph convolutional networks (GCN) recentl...</td>\n",
       "      <td>Update:\\r\\n\\r\\nI have read the rebuttal and th...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>graph convolutional network  gcn  recently pro...</td>\n",
       "      <td>update   read rebuttal revise manuscript  addi...</td>\n",
       "      <td>[[0.8052014]]</td>\n",
       "      <td>38.966292</td>\n",
       "      <td>6.006289</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>65</td>\n",
       "      <td>80.520140</td>\n",
       "      <td>159</td>\n",
       "      <td>1115</td>\n",
       "      <td>6.006289</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>The goal of this paper is to demonstrate a met...</td>\n",
       "      <td>The authors study compressing feed forward lay...</td>\n",
       "      <td>[[0.9087461]]</td>\n",
       "      <td>goal paper demonstrate method tensorizing neur...</td>\n",
       "      <td>author study compress fee forward layer use lo...</td>\n",
       "      <td>[[0.8880496]]</td>\n",
       "      <td>26.723926</td>\n",
       "      <td>5.686275</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>46</td>\n",
       "      <td>88.804960</td>\n",
       "      <td>102</td>\n",
       "      <td>683</td>\n",
       "      <td>5.686275</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>We conduct a mathematical analysis on the Batc...</td>\n",
       "      <td>This paper attempts to analyze the gradient fl...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>conduct mathematical analysis batch normalizat...</td>\n",
       "      <td>paper attempt analyze gradient flow batchnorm ...</td>\n",
       "      <td>[[0.7407055]]</td>\n",
       "      <td>18.814332</td>\n",
       "      <td>5.688000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>58</td>\n",
       "      <td>74.070550</td>\n",
       "      <td>125</td>\n",
       "      <td>835</td>\n",
       "      <td>5.688000</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     Despite being impactful on a  variety of probl...   \n",
       "1     Sequence-to-sequence (Seq2Seq) models with att...   \n",
       "2     Learning policies for complex tasks that requi...   \n",
       "3     Recent work has demonstrated that neural netwo...   \n",
       "4     In high dimensions, the performance of nearest...   \n",
       "...                                                 ...   \n",
       "2743  \\emph{Truncated Backpropagation Through Time} ...   \n",
       "2744  Actor-critic methods solve reinforcement learn...   \n",
       "2745  The graph convolutional networks (GCN) recentl...   \n",
       "2746  The goal of this paper is to demonstrate a met...   \n",
       "2747  We conduct a mathematical analysis on the Batc...   \n",
       "\n",
       "                                                 review semantic_similarity  \\\n",
       "0     Summary:\\r\\n\\r\\nThe paper proposes a new regul...           999999999   \n",
       "1     The paper proposes a novel approach to integra...      [[0.89935327]]   \n",
       "2     This paper introduces an iterative method to b...      [[0.86849725]]   \n",
       "3     - The authors investigate a minimax formulatio...           999999999   \n",
       "4     \\r\\nThe context is indexing images with descri...           999999999   \n",
       "...                                                 ...                 ...   \n",
       "2743  This paper proposes stochastic determination m...      [[0.84886956]]   \n",
       "2744  The paper introduces a modified actor-critic a...      [[0.88001734]]   \n",
       "2745  Update:\\r\\n\\r\\nI have read the rebuttal and th...           999999999   \n",
       "2746  The authors study compressing feed forward lay...       [[0.9087461]]   \n",
       "2747  This paper attempts to analyze the gradient fl...           999999999   \n",
       "\n",
       "                                         abstract_clean  \\\n",
       "0     despite impactful  variety problem application...   \n",
       "1     sequence   sequence  seq2seq  model attention ...   \n",
       "2     learning policy complex task require multiple ...   \n",
       "3     recent work demonstrate neural network vulnera...   \n",
       "4     high dimension  performance near neighbor algo...   \n",
       "...                                                 ...   \n",
       "2743  emphtruncated backpropagation time   truncate ...   \n",
       "2744  actor  critic method solve reinforcement learn...   \n",
       "2745  graph convolutional network  gcn  recently pro...   \n",
       "2746  goal paper demonstrate method tensorizing neur...   \n",
       "2747  conduct mathematical analysis batch normalizat...   \n",
       "\n",
       "                                           review_clean  \\\n",
       "0     summary   paper propose new regularizer wgans ...   \n",
       "1     paper propose novel approach integrate languag...   \n",
       "2     paper introduce iterative method build hierarc...   \n",
       "3      author investigate minimax formulation deep n...   \n",
       "4      context index image descriptor vector obtain ...   \n",
       "...                                                 ...   \n",
       "2743  paper propose stochastic determination method ...   \n",
       "2744  paper introduce modify actor  critic algorithm...   \n",
       "2745  update   read rebuttal revise manuscript  addi...   \n",
       "2746  author study compress fee forward layer use lo...   \n",
       "2747  paper attempt analyze gradient flow batchnorm ...   \n",
       "\n",
       "     semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                       [[0.84920603]]            20.219680          5.711538   \n",
       "1                        [[0.8828948]]            47.344262          5.768519   \n",
       "2                          [[0.79635]]            13.491979          5.764706   \n",
       "3                        [[0.7955401]]            29.983389          5.782051   \n",
       "4                       [[0.79431677]]            20.686636          6.309091   \n",
       "...                                ...                  ...               ...   \n",
       "2743                    [[0.81641376]]            31.202941          6.913043   \n",
       "2744                    [[0.79477006]]            13.005803          5.389937   \n",
       "2745                     [[0.8052014]]            38.966292          6.006289   \n",
       "2746                     [[0.8880496]]            26.723926          5.686275   \n",
       "2747                     [[0.7407055]]            18.814332          5.688000   \n",
       "\n",
       "      avg_sen_len_abs  freq_words_gt_sen_len_abs  similarity_score  word_cnt  \\\n",
       "0          174.666667                         68         84.920603       161   \n",
       "1          181.750000                         51         88.289480       108   \n",
       "2          126.777778                         85         79.635000       170   \n",
       "3          131.250000                         76         79.554010       156   \n",
       "4          133.833333                         55         79.431677       105   \n",
       "...               ...                        ...               ...       ...   \n",
       "2743       140.888889                         81         81.641376       162   \n",
       "2744       112.111111                         71         79.477006       159   \n",
       "2745       123.000000                         65         80.520140       159   \n",
       "2746       135.800000                         46         88.804960       102   \n",
       "2747       208.000000                         58         74.070550       125   \n",
       "\n",
       "      char_len  avg_wrd_length  stopwords_cnt  num_len  \n",
       "0         1053        5.711538             60        0  \n",
       "1          730        5.768519             37        0  \n",
       "2         1149        5.764706             57        0  \n",
       "3         1057        5.782051             51        0  \n",
       "4          808        6.309091             35        0  \n",
       "...        ...             ...            ...      ...  \n",
       "2743      1276        6.913043             44        0  \n",
       "2744      1017        5.389937             61        0  \n",
       "2745      1115        6.006289             56        0  \n",
       "2746       683        5.686275             35        1  \n",
       "2747       835        5.688000             50        0  \n",
       "\n",
       "[2651 rows x 16 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.word_count(data,\"abstract\",\"word_cnt\")\n",
    "tf.char_count(data,\"abstract\",\"char_len\")\n",
    "tf.avg_word_length(data,\"abstract\",\"avg_wrd_length\")\n",
    "tf.stopwords_count(data,\"abstract\",\"stopwords_cnt\")\n",
    "tf.numerics_count(data,\"abstract\",\"num_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "df = data[columns].copy()\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#df = pd.DataFrame(x_scaled)\n",
    "df=pd.DataFrame(x_scaled, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.158154</td>\n",
       "      <td>0.433427</td>\n",
       "      <td>0.332977</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.635832</td>\n",
       "      <td>0.272997</td>\n",
       "      <td>0.300599</td>\n",
       "      <td>0.433427</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.383890</td>\n",
       "      <td>0.548823</td>\n",
       "      <td>0.582213</td>\n",
       "      <td>0.322148</td>\n",
       "      <td>0.893384</td>\n",
       "      <td>0.270030</td>\n",
       "      <td>0.358230</td>\n",
       "      <td>0.548823</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.149268</td>\n",
       "      <td>0.395442</td>\n",
       "      <td>0.353797</td>\n",
       "      <td>0.483221</td>\n",
       "      <td>0.708830</td>\n",
       "      <td>0.376855</td>\n",
       "      <td>0.452743</td>\n",
       "      <td>0.395442</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.097822</td>\n",
       "      <td>0.415990</td>\n",
       "      <td>0.524450</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.622376</td>\n",
       "      <td>0.302671</td>\n",
       "      <td>0.329184</td>\n",
       "      <td>0.415990</td>\n",
       "      <td>0.283582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.378934</td>\n",
       "      <td>0.497898</td>\n",
       "      <td>0.188570</td>\n",
       "      <td>0.221477</td>\n",
       "      <td>0.812229</td>\n",
       "      <td>0.169139</td>\n",
       "      <td>0.213462</td>\n",
       "      <td>0.497898</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "14             0.158154          0.433427         0.332977   \n",
       "15             0.383890          0.548823         0.582213   \n",
       "18             0.149268          0.395442         0.353797   \n",
       "20             0.097822          0.415990         0.524450   \n",
       "25             0.378934          0.497898         0.188570   \n",
       "\n",
       "    freq_words_gt_sen_len_abs  similarity_score  word_cnt  char_len  \\\n",
       "14                   0.328859          0.635832  0.272997  0.300599   \n",
       "15                   0.322148          0.893384  0.270030  0.358230   \n",
       "18                   0.483221          0.708830  0.376855  0.452743   \n",
       "20                   0.328859          0.622376  0.302671  0.329184   \n",
       "25                   0.221477          0.812229  0.169139  0.213462   \n",
       "\n",
       "    avg_wrd_length  stopwords_cnt  num_len  \n",
       "14        0.433427       0.208955      0.0  \n",
       "15        0.548823       0.253731      0.0  \n",
       "18        0.395442       0.402985      0.0  \n",
       "20        0.415990       0.283582      0.0  \n",
       "25        0.497898       0.179104      0.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = data.sample(frac=0.8, random_state = 1)\n",
    "test_set = data.loc[~data.index.isin(training_set.index)]\n",
    "test_set.head()\n",
    "\n",
    "training_set = df.sample(frac=0.8, random_state = 1)\n",
    "test_set = df.loc[~df.index.isin(training_set.index)]\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['yules_i_measure_abs', 'avg_word_len_abs',\n",
    "       'avg_sen_len_abs', 'freq_words_gt_sen_len_abs']\n",
    "\n",
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "\n",
    "\n",
    "# training_data = training_set.as_matrix(columns = data_columns)\n",
    "training_data = training_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(training_data)\n",
    "training_data[nan_locs] = 0\n",
    "\n",
    "training_target = training_set['similarity_score'].values\n",
    "nan_locs = np.isnan(training_target)\n",
    "training_target[nan_locs] = 0\n",
    "\n",
    "test_data = test_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(test_data)\n",
    "test_data[nan_locs] = 0\n",
    "\n",
    "test_target = test_set['similarity_score'].values\n",
    "nan_locs = np.isnan(test_target)\n",
    "test_target[nan_locs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.10530210119515382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor()\n",
    "reg.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(reg.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.13754578287526942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(svr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.9213494517570293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(kr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.06190389001148411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(dt.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.10114161785505382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(rf.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.16823041850908915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(gb.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.1423808173212131\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pr = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "              ('linear', LinearRegression(fit_intercept = False))])\n",
    "\n",
    "pr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(pr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005972665472915306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lm = Lasso()\n",
    "lm.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(lm.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005972665472915306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(en.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005972665472915306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "ll = LassoLars()\n",
    "ll.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(ll.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.10045775710986149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(br.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.011616826964932825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(sgd.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
