{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def compute_collocation_score(sentence_one, sentence_two, option):\n",
    "    if option == \"bi\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = BigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = BigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    elif option == \"tri\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = TrigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = TrigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def vectorize(sentence, vocabulary):\n",
    "    result = [sentence.split().count(i) for i in vocabulary]\n",
    "    return result\n",
    "\n",
    "def convert_words_to_vectors(sentence):\n",
    "    vectorized_sentence = []\n",
    "    vocabulary = sorted(set(chain(*[words.lower().split() for words in sentence])))\n",
    "    for words in sentence:\n",
    "        vectorized_sentence.append((words, vectorize(words, vocabulary)))\n",
    "    return vectorized_sentence, vocabulary\n",
    "\n",
    "def dot_product_of_vectors(vector_one, vector_two):\n",
    "    result = np.dot(vector_one, vector_two) / (sqrt(np.dot(vector_one, vector_one)) * sqrt(np.dot(vector_two, vector_two)))\n",
    "    return result\n",
    "\n",
    "def cosine_sim(sentence_one, sentence_two):\n",
    "    sentences = [sentence_one, sentence_two]\n",
    "    corpus, vocabulary = convert_words_to_vectors(sentences)\n",
    "    similarity = [dot_product_of_vectors(a[1], b[1]) for a, b in product(corpus, corpus)]\n",
    "    return similarity[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"reviews_scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We point out important problems with the commo...</td>\n",
       "      <td>The authors propose a new measure to capture t...</td>\n",
       "      <td>0.614176</td>\n",
       "      <td>37.969388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine translation has recently achieved impr...</td>\n",
       "      <td>This paper describes an approach to train a ne...</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>26.670034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We build on auto-encoding sequential Monte Car...</td>\n",
       "      <td>Update:\\nOn further consideration (and reading...</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>22.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep learning networks have achieved state-of-...</td>\n",
       "      <td>The paper aims at improving the accuracy of a ...</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>33.580420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every second, innumerable text data, including...</td>\n",
       "      <td>* PAPER SUMMARY *\\n\\nThis paper proposes a sia...</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>51.265823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  We point out important problems with the commo...   \n",
       "1  Machine translation has recently achieved impr...   \n",
       "2  We build on auto-encoding sequential Monte Car...   \n",
       "3  Deep learning networks have achieved state-of-...   \n",
       "4  Every second, innumerable text data, including...   \n",
       "\n",
       "                                              review  similarity_score  \\\n",
       "0  The authors propose a new measure to capture t...          0.614176   \n",
       "1  This paper describes an approach to train a ne...          0.581566   \n",
       "2  Update:\\nOn further consideration (and reading...          0.505319   \n",
       "3  The paper aims at improving the accuracy of a ...          0.452746   \n",
       "4  * PAPER SUMMARY *\\n\\nThis paper proposes a sia...          0.424581   \n",
       "\n",
       "   yules_i_measure_abs  \n",
       "0            37.969388  \n",
       "1            26.670034  \n",
       "2            22.224490  \n",
       "3            33.580420  \n",
       "4            51.265823  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We point out important problems with the commo...</td>\n",
       "      <td>The authors propose a new measure to capture t...</td>\n",
       "      <td>0.614176</td>\n",
       "      <td>37.969388</td>\n",
       "      <td>5.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine translation has recently achieved impr...</td>\n",
       "      <td>This paper describes an approach to train a ne...</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>26.670034</td>\n",
       "      <td>5.513333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We build on auto-encoding sequential Monte Car...</td>\n",
       "      <td>Update:\\nOn further consideration (and reading...</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>22.224490</td>\n",
       "      <td>6.084906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep learning networks have achieved state-of-...</td>\n",
       "      <td>The paper aims at improving the accuracy of a ...</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>33.580420</td>\n",
       "      <td>6.158228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every second, innumerable text data, including...</td>\n",
       "      <td>* PAPER SUMMARY *\\n\\nThis paper proposes a sia...</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>51.265823</td>\n",
       "      <td>5.748031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  We point out important problems with the commo...   \n",
       "1  Machine translation has recently achieved impr...   \n",
       "2  We build on auto-encoding sequential Monte Car...   \n",
       "3  Deep learning networks have achieved state-of-...   \n",
       "4  Every second, innumerable text data, including...   \n",
       "\n",
       "                                              review  similarity_score  \\\n",
       "0  The authors propose a new measure to capture t...          0.614176   \n",
       "1  This paper describes an approach to train a ne...          0.581566   \n",
       "2  Update:\\nOn further consideration (and reading...          0.505319   \n",
       "3  The paper aims at improving the accuracy of a ...          0.452746   \n",
       "4  * PAPER SUMMARY *\\n\\nThis paper proposes a sia...          0.424581   \n",
       "\n",
       "   yules_i_measure_abs  avg_word_len_abs  \n",
       "0            37.969388          5.705882  \n",
       "1            26.670034          5.513333  \n",
       "2            22.224490          6.084906  \n",
       "3            33.580420          6.158228  \n",
       "4            51.265823          5.748031  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We point out important problems with the commo...</td>\n",
       "      <td>The authors propose a new measure to capture t...</td>\n",
       "      <td>0.614176</td>\n",
       "      <td>37.969388</td>\n",
       "      <td>5.705882</td>\n",
       "      <td>141.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine translation has recently achieved impr...</td>\n",
       "      <td>This paper describes an approach to train a ne...</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>26.670034</td>\n",
       "      <td>5.513333</td>\n",
       "      <td>161.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We build on auto-encoding sequential Monte Car...</td>\n",
       "      <td>Update:\\nOn further consideration (and reading...</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>22.224490</td>\n",
       "      <td>6.084906</td>\n",
       "      <td>186.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep learning networks have achieved state-of-...</td>\n",
       "      <td>The paper aims at improving the accuracy of a ...</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>33.580420</td>\n",
       "      <td>6.158228</td>\n",
       "      <td>160.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every second, innumerable text data, including...</td>\n",
       "      <td>* PAPER SUMMARY *\\n\\nThis paper proposes a sia...</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>51.265823</td>\n",
       "      <td>5.748031</td>\n",
       "      <td>213.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  We point out important problems with the commo...   \n",
       "1  Machine translation has recently achieved impr...   \n",
       "2  We build on auto-encoding sequential Monte Car...   \n",
       "3  Deep learning networks have achieved state-of-...   \n",
       "4  Every second, innumerable text data, including...   \n",
       "\n",
       "                                              review  similarity_score  \\\n",
       "0  The authors propose a new measure to capture t...          0.614176   \n",
       "1  This paper describes an approach to train a ne...          0.581566   \n",
       "2  Update:\\nOn further consideration (and reading...          0.505319   \n",
       "3  The paper aims at improving the accuracy of a ...          0.452746   \n",
       "4  * PAPER SUMMARY *\\n\\nThis paper proposes a sia...          0.424581   \n",
       "\n",
       "   yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \n",
       "0            37.969388          5.705882       141.500000  \n",
       "1            26.670034          5.513333       161.833333  \n",
       "2            22.224490          6.084906       186.750000  \n",
       "3            33.580420          6.158228       160.571429  \n",
       "4            51.265823          5.748031       213.500000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We point out important problems with the commo...</td>\n",
       "      <td>The authors propose a new measure to capture t...</td>\n",
       "      <td>0.614176</td>\n",
       "      <td>37.969388</td>\n",
       "      <td>5.705882</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine translation has recently achieved impr...</td>\n",
       "      <td>This paper describes an approach to train a ne...</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>26.670034</td>\n",
       "      <td>5.513333</td>\n",
       "      <td>161.833333</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We build on auto-encoding sequential Monte Car...</td>\n",
       "      <td>Update:\\nOn further consideration (and reading...</td>\n",
       "      <td>0.505319</td>\n",
       "      <td>22.224490</td>\n",
       "      <td>6.084906</td>\n",
       "      <td>186.750000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deep learning networks have achieved state-of-...</td>\n",
       "      <td>The paper aims at improving the accuracy of a ...</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>33.580420</td>\n",
       "      <td>6.158228</td>\n",
       "      <td>160.571429</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every second, innumerable text data, including...</td>\n",
       "      <td>* PAPER SUMMARY *\\n\\nThis paper proposes a sia...</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>51.265823</td>\n",
       "      <td>5.748031</td>\n",
       "      <td>213.500000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  We point out important problems with the commo...   \n",
       "1  Machine translation has recently achieved impr...   \n",
       "2  We build on auto-encoding sequential Monte Car...   \n",
       "3  Deep learning networks have achieved state-of-...   \n",
       "4  Every second, innumerable text data, including...   \n",
       "\n",
       "                                              review  similarity_score  \\\n",
       "0  The authors propose a new measure to capture t...          0.614176   \n",
       "1  This paper describes an approach to train a ne...          0.581566   \n",
       "2  Update:\\nOn further consideration (and reading...          0.505319   \n",
       "3  The paper aims at improving the accuracy of a ...          0.452746   \n",
       "4  * PAPER SUMMARY *\\n\\nThis paper proposes a sia...          0.424581   \n",
       "\n",
       "   yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "0            37.969388          5.705882       141.500000   \n",
       "1            26.670034          5.513333       161.833333   \n",
       "2            22.224490          6.084906       186.750000   \n",
       "3            33.580420          6.158228       160.571429   \n",
       "4            51.265823          5.748031       213.500000   \n",
       "\n",
       "   freq_words_gt_sen_len_abs  \n",
       "0                         40  \n",
       "1                         68  \n",
       "2                         45  \n",
       "3                         68  \n",
       "4                         51  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv(\"regression_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
