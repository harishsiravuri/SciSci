{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import textfeatures as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def compute_collocation_score(sentence_one, sentence_two, option):\n",
    "    if option == \"bi\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = BigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = BigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    elif option == \"tri\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = TrigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = TrigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def vectorize(sentence, vocabulary):\n",
    "    result = [sentence.split().count(i) for i in vocabulary]\n",
    "    return result\n",
    "\n",
    "def convert_words_to_vectors(sentence):\n",
    "    vectorized_sentence = []\n",
    "    vocabulary = sorted(set(chain(*[words.lower().split() for words in sentence])))\n",
    "    for words in sentence:\n",
    "        vectorized_sentence.append((words, vectorize(words, vocabulary)))\n",
    "    return vectorized_sentence, vocabulary\n",
    "\n",
    "def dot_product_of_vectors(vector_one, vector_two):\n",
    "    result = np.dot(vector_one, vector_two) / (sqrt(np.dot(vector_one, vector_one)) * sqrt(np.dot(vector_two, vector_two)))\n",
    "    return result\n",
    "\n",
    "def cosine_sim(sentence_one, sentence_two):\n",
    "    sentences = [sentence_one, sentence_two]\n",
    "    corpus, vocabulary = convert_words_to_vectors(sentences)\n",
    "    similarity = [dot_product_of_vectors(a[1], b[1]) for a, b in product(corpus, corpus)]\n",
    "    return similarity[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"reviews_scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite being impactful on a  variety of probl...</td>\n",
       "      <td>Summary:\\r\\n\\r\\nThe paper proposes a new regul...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>despite impactful  variety problem application...</td>\n",
       "      <td>summary   paper propose new regularizer wgans ...</td>\n",
       "      <td>[[0.84920603]]</td>\n",
       "      <td>20.219680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence-to-sequence (Seq2Seq) models with att...</td>\n",
       "      <td>The paper proposes a novel approach to integra...</td>\n",
       "      <td>[[0.89935327]]</td>\n",
       "      <td>sequence   sequence  seq2seq  model attention ...</td>\n",
       "      <td>paper propose novel approach integrate languag...</td>\n",
       "      <td>[[0.8828948]]</td>\n",
       "      <td>47.344262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning policies for complex tasks that requi...</td>\n",
       "      <td>This paper introduces an iterative method to b...</td>\n",
       "      <td>[[0.86849725]]</td>\n",
       "      <td>learning policy complex task require multiple ...</td>\n",
       "      <td>paper introduce iterative method build hierarc...</td>\n",
       "      <td>[[0.79635]]</td>\n",
       "      <td>13.491979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent work has demonstrated that neural netwo...</td>\n",
       "      <td>- The authors investigate a minimax formulatio...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recent work demonstrate neural network vulnera...</td>\n",
       "      <td>author investigate minimax formulation deep n...</td>\n",
       "      <td>[[0.7955401]]</td>\n",
       "      <td>29.983389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In high dimensions, the performance of nearest...</td>\n",
       "      <td>\\r\\nThe context is indexing images with descri...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>high dimension  performance near neighbor algo...</td>\n",
       "      <td>context index image descriptor vector obtain ...</td>\n",
       "      <td>[[0.79431677]]</td>\n",
       "      <td>20.686636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Despite being impactful on a  variety of probl...   \n",
       "1  Sequence-to-sequence (Seq2Seq) models with att...   \n",
       "2  Learning policies for complex tasks that requi...   \n",
       "3  Recent work has demonstrated that neural netwo...   \n",
       "4  In high dimensions, the performance of nearest...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  Summary:\\r\\n\\r\\nThe paper proposes a new regul...           999999999   \n",
       "1  The paper proposes a novel approach to integra...      [[0.89935327]]   \n",
       "2  This paper introduces an iterative method to b...      [[0.86849725]]   \n",
       "3  - The authors investigate a minimax formulatio...           999999999   \n",
       "4  \\r\\nThe context is indexing images with descri...           999999999   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  despite impactful  variety problem application...   \n",
       "1  sequence   sequence  seq2seq  model attention ...   \n",
       "2  learning policy complex task require multiple ...   \n",
       "3  recent work demonstrate neural network vulnera...   \n",
       "4  high dimension  performance near neighbor algo...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  summary   paper propose new regularizer wgans ...   \n",
       "1  paper propose novel approach integrate languag...   \n",
       "2  paper introduce iterative method build hierarc...   \n",
       "3   author investigate minimax formulation deep n...   \n",
       "4   context index image descriptor vector obtain ...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  \n",
       "0                    [[0.84920603]]            20.219680  \n",
       "1                     [[0.8828948]]            47.344262  \n",
       "2                       [[0.79635]]            13.491979  \n",
       "3                     [[0.7955401]]            29.983389  \n",
       "4                    [[0.79431677]]            20.686636  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite being impactful on a  variety of probl...</td>\n",
       "      <td>Summary:\\r\\n\\r\\nThe paper proposes a new regul...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>despite impactful  variety problem application...</td>\n",
       "      <td>summary   paper propose new regularizer wgans ...</td>\n",
       "      <td>[[0.84920603]]</td>\n",
       "      <td>20.219680</td>\n",
       "      <td>5.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence-to-sequence (Seq2Seq) models with att...</td>\n",
       "      <td>The paper proposes a novel approach to integra...</td>\n",
       "      <td>[[0.89935327]]</td>\n",
       "      <td>sequence   sequence  seq2seq  model attention ...</td>\n",
       "      <td>paper propose novel approach integrate languag...</td>\n",
       "      <td>[[0.8828948]]</td>\n",
       "      <td>47.344262</td>\n",
       "      <td>5.768519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning policies for complex tasks that requi...</td>\n",
       "      <td>This paper introduces an iterative method to b...</td>\n",
       "      <td>[[0.86849725]]</td>\n",
       "      <td>learning policy complex task require multiple ...</td>\n",
       "      <td>paper introduce iterative method build hierarc...</td>\n",
       "      <td>[[0.79635]]</td>\n",
       "      <td>13.491979</td>\n",
       "      <td>5.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent work has demonstrated that neural netwo...</td>\n",
       "      <td>- The authors investigate a minimax formulatio...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recent work demonstrate neural network vulnera...</td>\n",
       "      <td>author investigate minimax formulation deep n...</td>\n",
       "      <td>[[0.7955401]]</td>\n",
       "      <td>29.983389</td>\n",
       "      <td>5.782051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In high dimensions, the performance of nearest...</td>\n",
       "      <td>\\r\\nThe context is indexing images with descri...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>high dimension  performance near neighbor algo...</td>\n",
       "      <td>context index image descriptor vector obtain ...</td>\n",
       "      <td>[[0.79431677]]</td>\n",
       "      <td>20.686636</td>\n",
       "      <td>6.309091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Despite being impactful on a  variety of probl...   \n",
       "1  Sequence-to-sequence (Seq2Seq) models with att...   \n",
       "2  Learning policies for complex tasks that requi...   \n",
       "3  Recent work has demonstrated that neural netwo...   \n",
       "4  In high dimensions, the performance of nearest...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  Summary:\\r\\n\\r\\nThe paper proposes a new regul...           999999999   \n",
       "1  The paper proposes a novel approach to integra...      [[0.89935327]]   \n",
       "2  This paper introduces an iterative method to b...      [[0.86849725]]   \n",
       "3  - The authors investigate a minimax formulatio...           999999999   \n",
       "4  \\r\\nThe context is indexing images with descri...           999999999   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  despite impactful  variety problem application...   \n",
       "1  sequence   sequence  seq2seq  model attention ...   \n",
       "2  learning policy complex task require multiple ...   \n",
       "3  recent work demonstrate neural network vulnera...   \n",
       "4  high dimension  performance near neighbor algo...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  summary   paper propose new regularizer wgans ...   \n",
       "1  paper propose novel approach integrate languag...   \n",
       "2  paper introduce iterative method build hierarc...   \n",
       "3   author investigate minimax formulation deep n...   \n",
       "4   context index image descriptor vector obtain ...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \n",
       "0                    [[0.84920603]]            20.219680          5.711538  \n",
       "1                     [[0.8828948]]            47.344262          5.768519  \n",
       "2                       [[0.79635]]            13.491979          5.764706  \n",
       "3                     [[0.7955401]]            29.983389          5.782051  \n",
       "4                    [[0.79431677]]            20.686636          6.309091  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite being impactful on a  variety of probl...</td>\n",
       "      <td>Summary:\\r\\n\\r\\nThe paper proposes a new regul...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>despite impactful  variety problem application...</td>\n",
       "      <td>summary   paper propose new regularizer wgans ...</td>\n",
       "      <td>[[0.84920603]]</td>\n",
       "      <td>20.219680</td>\n",
       "      <td>5.711538</td>\n",
       "      <td>174.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence-to-sequence (Seq2Seq) models with att...</td>\n",
       "      <td>The paper proposes a novel approach to integra...</td>\n",
       "      <td>[[0.89935327]]</td>\n",
       "      <td>sequence   sequence  seq2seq  model attention ...</td>\n",
       "      <td>paper propose novel approach integrate languag...</td>\n",
       "      <td>[[0.8828948]]</td>\n",
       "      <td>47.344262</td>\n",
       "      <td>5.768519</td>\n",
       "      <td>181.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning policies for complex tasks that requi...</td>\n",
       "      <td>This paper introduces an iterative method to b...</td>\n",
       "      <td>[[0.86849725]]</td>\n",
       "      <td>learning policy complex task require multiple ...</td>\n",
       "      <td>paper introduce iterative method build hierarc...</td>\n",
       "      <td>[[0.79635]]</td>\n",
       "      <td>13.491979</td>\n",
       "      <td>5.764706</td>\n",
       "      <td>126.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent work has demonstrated that neural netwo...</td>\n",
       "      <td>- The authors investigate a minimax formulatio...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recent work demonstrate neural network vulnera...</td>\n",
       "      <td>author investigate minimax formulation deep n...</td>\n",
       "      <td>[[0.7955401]]</td>\n",
       "      <td>29.983389</td>\n",
       "      <td>5.782051</td>\n",
       "      <td>131.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In high dimensions, the performance of nearest...</td>\n",
       "      <td>\\r\\nThe context is indexing images with descri...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>high dimension  performance near neighbor algo...</td>\n",
       "      <td>context index image descriptor vector obtain ...</td>\n",
       "      <td>[[0.79431677]]</td>\n",
       "      <td>20.686636</td>\n",
       "      <td>6.309091</td>\n",
       "      <td>133.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Despite being impactful on a  variety of probl...   \n",
       "1  Sequence-to-sequence (Seq2Seq) models with att...   \n",
       "2  Learning policies for complex tasks that requi...   \n",
       "3  Recent work has demonstrated that neural netwo...   \n",
       "4  In high dimensions, the performance of nearest...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  Summary:\\r\\n\\r\\nThe paper proposes a new regul...           999999999   \n",
       "1  The paper proposes a novel approach to integra...      [[0.89935327]]   \n",
       "2  This paper introduces an iterative method to b...      [[0.86849725]]   \n",
       "3  - The authors investigate a minimax formulatio...           999999999   \n",
       "4  \\r\\nThe context is indexing images with descri...           999999999   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  despite impactful  variety problem application...   \n",
       "1  sequence   sequence  seq2seq  model attention ...   \n",
       "2  learning policy complex task require multiple ...   \n",
       "3  recent work demonstrate neural network vulnera...   \n",
       "4  high dimension  performance near neighbor algo...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  summary   paper propose new regularizer wgans ...   \n",
       "1  paper propose novel approach integrate languag...   \n",
       "2  paper introduce iterative method build hierarc...   \n",
       "3   author investigate minimax formulation deep n...   \n",
       "4   context index image descriptor vector obtain ...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                    [[0.84920603]]            20.219680          5.711538   \n",
       "1                     [[0.8828948]]            47.344262          5.768519   \n",
       "2                       [[0.79635]]            13.491979          5.764706   \n",
       "3                     [[0.7955401]]            29.983389          5.782051   \n",
       "4                    [[0.79431677]]            20.686636          6.309091   \n",
       "\n",
       "   avg_sen_len_abs  \n",
       "0       174.666667  \n",
       "1       181.750000  \n",
       "2       126.777778  \n",
       "3       131.250000  \n",
       "4       133.833333  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite being impactful on a  variety of probl...</td>\n",
       "      <td>Summary:\\r\\n\\r\\nThe paper proposes a new regul...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>despite impactful  variety problem application...</td>\n",
       "      <td>summary   paper propose new regularizer wgans ...</td>\n",
       "      <td>[[0.84920603]]</td>\n",
       "      <td>20.219680</td>\n",
       "      <td>5.711538</td>\n",
       "      <td>174.666667</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sequence-to-sequence (Seq2Seq) models with att...</td>\n",
       "      <td>The paper proposes a novel approach to integra...</td>\n",
       "      <td>[[0.89935327]]</td>\n",
       "      <td>sequence   sequence  seq2seq  model attention ...</td>\n",
       "      <td>paper propose novel approach integrate languag...</td>\n",
       "      <td>[[0.8828948]]</td>\n",
       "      <td>47.344262</td>\n",
       "      <td>5.768519</td>\n",
       "      <td>181.750000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Learning policies for complex tasks that requi...</td>\n",
       "      <td>This paper introduces an iterative method to b...</td>\n",
       "      <td>[[0.86849725]]</td>\n",
       "      <td>learning policy complex task require multiple ...</td>\n",
       "      <td>paper introduce iterative method build hierarc...</td>\n",
       "      <td>[[0.79635]]</td>\n",
       "      <td>13.491979</td>\n",
       "      <td>5.764706</td>\n",
       "      <td>126.777778</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recent work has demonstrated that neural netwo...</td>\n",
       "      <td>- The authors investigate a minimax formulatio...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recent work demonstrate neural network vulnera...</td>\n",
       "      <td>author investigate minimax formulation deep n...</td>\n",
       "      <td>[[0.7955401]]</td>\n",
       "      <td>29.983389</td>\n",
       "      <td>5.782051</td>\n",
       "      <td>131.250000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In high dimensions, the performance of nearest...</td>\n",
       "      <td>\\r\\nThe context is indexing images with descri...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>high dimension  performance near neighbor algo...</td>\n",
       "      <td>context index image descriptor vector obtain ...</td>\n",
       "      <td>[[0.79431677]]</td>\n",
       "      <td>20.686636</td>\n",
       "      <td>6.309091</td>\n",
       "      <td>133.833333</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Despite being impactful on a  variety of probl...   \n",
       "1  Sequence-to-sequence (Seq2Seq) models with att...   \n",
       "2  Learning policies for complex tasks that requi...   \n",
       "3  Recent work has demonstrated that neural netwo...   \n",
       "4  In high dimensions, the performance of nearest...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  Summary:\\r\\n\\r\\nThe paper proposes a new regul...           999999999   \n",
       "1  The paper proposes a novel approach to integra...      [[0.89935327]]   \n",
       "2  This paper introduces an iterative method to b...      [[0.86849725]]   \n",
       "3  - The authors investigate a minimax formulatio...           999999999   \n",
       "4  \\r\\nThe context is indexing images with descri...           999999999   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  despite impactful  variety problem application...   \n",
       "1  sequence   sequence  seq2seq  model attention ...   \n",
       "2  learning policy complex task require multiple ...   \n",
       "3  recent work demonstrate neural network vulnera...   \n",
       "4  high dimension  performance near neighbor algo...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  summary   paper propose new regularizer wgans ...   \n",
       "1  paper propose novel approach integrate languag...   \n",
       "2  paper introduce iterative method build hierarc...   \n",
       "3   author investigate minimax formulation deep n...   \n",
       "4   context index image descriptor vector obtain ...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                    [[0.84920603]]            20.219680          5.711538   \n",
       "1                     [[0.8828948]]            47.344262          5.768519   \n",
       "2                       [[0.79635]]            13.491979          5.764706   \n",
       "3                     [[0.7955401]]            29.983389          5.782051   \n",
       "4                    [[0.79431677]]            20.686636          6.309091   \n",
       "\n",
       "   avg_sen_len_abs  freq_words_gt_sen_len_abs  \n",
       "0       174.666667                         68  \n",
       "1       181.750000                         51  \n",
       "2       126.777778                         85  \n",
       "3       131.250000                         76  \n",
       "4       133.833333                         55  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.to_csv(\"regression_data_scibert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ca99b957960f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"word_cnt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"char_len\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_word_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"avg_wrd_length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"stopwords_cnt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerics_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"abstract\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"num_len\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
