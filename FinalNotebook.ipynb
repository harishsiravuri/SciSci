{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import textfeatures as tf\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Labeled text classification datasets are typic...</td>\n",
       "      <td>This paper proposes a language independent tex...</td>\n",
       "      <td>[[0.9102851]]</td>\n",
       "      <td>labeled text classification dataset typically ...</td>\n",
       "      <td>paper propose language independent text encode...</td>\n",
       "      <td>[[0.8852489]]</td>\n",
       "      <td>13.585965</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>116.888889</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We propose and evaluate new techniques for com...</td>\n",
       "      <td>The problem considered in the paper is of comp...</td>\n",
       "      <td>[[0.88059396]]</td>\n",
       "      <td>propose evaluate new technique compress speed ...</td>\n",
       "      <td>problem consider paper compress large network ...</td>\n",
       "      <td>[[0.8604121]]</td>\n",
       "      <td>31.352227</td>\n",
       "      <td>5.517730</td>\n",
       "      <td>182.800000</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep neural networks have become the state-of-...</td>\n",
       "      <td>The authors cast some of the most recent CNN d...</td>\n",
       "      <td>[[0.90264124]]</td>\n",
       "      <td>deep neural network state    art model numerou...</td>\n",
       "      <td>author cast recent cnn design approximate solu...</td>\n",
       "      <td>[[0.88722855]]</td>\n",
       "      <td>20.851064</td>\n",
       "      <td>6.115942</td>\n",
       "      <td>139.285714</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recurrent Neural Networks (RNNs) continue to s...</td>\n",
       "      <td>UPDATE: Following the author's response I've i...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>recurrent neural networks  rnns  continue  out...</td>\n",
       "      <td>update  following author response pron increas...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>32.060606</td>\n",
       "      <td>6.028986</td>\n",
       "      <td>137.714286</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the pursuit of increasingly intelligent lea...</td>\n",
       "      <td>The paper presents a new policy gradient techn...</td>\n",
       "      <td>[[0.85638505]]</td>\n",
       "      <td>pursuit increasingly intelligent learn system ...</td>\n",
       "      <td>paper present new policy gradient technique le...</td>\n",
       "      <td>[[0.7674166]]</td>\n",
       "      <td>31.431151</td>\n",
       "      <td>5.743455</td>\n",
       "      <td>142.444444</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Labeled text classification datasets are typic...   \n",
       "1  We propose and evaluate new techniques for com...   \n",
       "2  Deep neural networks have become the state-of-...   \n",
       "3  Recurrent Neural Networks (RNNs) continue to s...   \n",
       "4  In the pursuit of increasingly intelligent lea...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  This paper proposes a language independent tex...       [[0.9102851]]   \n",
       "1  The problem considered in the paper is of comp...      [[0.88059396]]   \n",
       "2  The authors cast some of the most recent CNN d...      [[0.90264124]]   \n",
       "3  UPDATE: Following the author's response I've i...           999999999   \n",
       "4  The paper presents a new policy gradient techn...      [[0.85638505]]   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  labeled text classification dataset typically ...   \n",
       "1  propose evaluate new technique compress speed ...   \n",
       "2  deep neural network state    art model numerou...   \n",
       "3  recurrent neural networks  rnns  continue  out...   \n",
       "4  pursuit increasingly intelligent learn system ...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  paper propose language independent text encode...   \n",
       "1  problem consider paper compress large network ...   \n",
       "2  author cast recent cnn design approximate solu...   \n",
       "3  update  following author response pron increas...   \n",
       "4  paper present new policy gradient technique le...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                     [[0.8852489]]            13.585965          5.086207   \n",
       "1                     [[0.8604121]]            31.352227          5.517730   \n",
       "2                    [[0.88722855]]            20.851064          6.115942   \n",
       "3                         999999999            32.060606          6.028986   \n",
       "4                     [[0.7674166]]            31.431151          5.743455   \n",
       "\n",
       "   avg_sen_len_abs  freq_words_gt_sen_len_abs  \n",
       "0       116.888889                         63  \n",
       "1       182.800000                         65  \n",
       "2       139.285714                        124  \n",
       "3       137.714286                         53  \n",
       "4       142.444444                         97  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data['similarity_score'] = pd.to_numeric(data['semantic_similarity_aftercleaning'].str.replace('[','').str.replace(']',''))\n",
    "data = data[data['similarity_score'] != 999999999.0].copy()\n",
    "data['similarity_score'] = data['similarity_score'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Labeled text classification datasets are typic...</td>\n",
       "      <td>This paper proposes a language independent tex...</td>\n",
       "      <td>[[0.9102851]]</td>\n",
       "      <td>labeled text classification dataset typically ...</td>\n",
       "      <td>paper propose language independent text encode...</td>\n",
       "      <td>[[0.8852489]]</td>\n",
       "      <td>13.585965</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>116.888889</td>\n",
       "      <td>63</td>\n",
       "      <td>88.524890</td>\n",
       "      <td>176</td>\n",
       "      <td>1060</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We propose and evaluate new techniques for com...</td>\n",
       "      <td>The problem considered in the paper is of comp...</td>\n",
       "      <td>[[0.88059396]]</td>\n",
       "      <td>propose evaluate new technique compress speed ...</td>\n",
       "      <td>problem consider paper compress large network ...</td>\n",
       "      <td>[[0.8604121]]</td>\n",
       "      <td>31.352227</td>\n",
       "      <td>5.517730</td>\n",
       "      <td>182.800000</td>\n",
       "      <td>65</td>\n",
       "      <td>86.041210</td>\n",
       "      <td>141</td>\n",
       "      <td>918</td>\n",
       "      <td>5.517730</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep neural networks have become the state-of-...</td>\n",
       "      <td>The authors cast some of the most recent CNN d...</td>\n",
       "      <td>[[0.90264124]]</td>\n",
       "      <td>deep neural network state    art model numerou...</td>\n",
       "      <td>author cast recent cnn design approximate solu...</td>\n",
       "      <td>[[0.88722855]]</td>\n",
       "      <td>20.851064</td>\n",
       "      <td>6.115942</td>\n",
       "      <td>139.285714</td>\n",
       "      <td>124</td>\n",
       "      <td>88.722855</td>\n",
       "      <td>276</td>\n",
       "      <td>1963</td>\n",
       "      <td>6.115942</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the pursuit of increasingly intelligent lea...</td>\n",
       "      <td>The paper presents a new policy gradient techn...</td>\n",
       "      <td>[[0.85638505]]</td>\n",
       "      <td>pursuit increasingly intelligent learn system ...</td>\n",
       "      <td>paper present new policy gradient technique le...</td>\n",
       "      <td>[[0.7674166]]</td>\n",
       "      <td>31.431151</td>\n",
       "      <td>5.743455</td>\n",
       "      <td>142.444444</td>\n",
       "      <td>97</td>\n",
       "      <td>76.741660</td>\n",
       "      <td>194</td>\n",
       "      <td>1290</td>\n",
       "      <td>5.743455</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We exploit a recently derived inversion scheme...</td>\n",
       "      <td>In summary, the paper is based on a recent wor...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>exploit recently derive inversion scheme arbit...</td>\n",
       "      <td>summary  paper base recent work balestriero  b...</td>\n",
       "      <td>[[0.7782445]]</td>\n",
       "      <td>37.320388</td>\n",
       "      <td>6.070588</td>\n",
       "      <td>119.800000</td>\n",
       "      <td>42</td>\n",
       "      <td>77.824450</td>\n",
       "      <td>86</td>\n",
       "      <td>603</td>\n",
       "      <td>6.070588</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>Convolutional neural networks (CNNs) are inher...</td>\n",
       "      <td>This paper presents a new convolutional networ...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>convolutional neural network  cnns  inherently...</td>\n",
       "      <td>paper present new convolutional network archit...</td>\n",
       "      <td>[[0.7938315]]</td>\n",
       "      <td>17.384127</td>\n",
       "      <td>6.038760</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>53</td>\n",
       "      <td>79.383150</td>\n",
       "      <td>129</td>\n",
       "      <td>907</td>\n",
       "      <td>6.038760</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>Recent DNN pruning algorithms have succeeded i...</td>\n",
       "      <td>This paper casts the pruning optimization prob...</td>\n",
       "      <td>[[0.87137043]]</td>\n",
       "      <td>recent dnn prune algorithm succeed reduce numb...</td>\n",
       "      <td>paper cast prune optimization problem nettrim ...</td>\n",
       "      <td>[[0.83231395]]</td>\n",
       "      <td>43.742574</td>\n",
       "      <td>5.720588</td>\n",
       "      <td>151.333333</td>\n",
       "      <td>68</td>\n",
       "      <td>83.231395</td>\n",
       "      <td>136</td>\n",
       "      <td>913</td>\n",
       "      <td>5.720588</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>Memory Network based models have shown a remar...</td>\n",
       "      <td>The paper proposes to address the quadratic me...</td>\n",
       "      <td>[[0.8944535]]</td>\n",
       "      <td>memory network base model remarkable progress ...</td>\n",
       "      <td>paper propose address quadratic memory  time r...</td>\n",
       "      <td>[[0.88223743]]</td>\n",
       "      <td>25.137931</td>\n",
       "      <td>5.945312</td>\n",
       "      <td>111.500000</td>\n",
       "      <td>71</td>\n",
       "      <td>88.223743</td>\n",
       "      <td>127</td>\n",
       "      <td>899</td>\n",
       "      <td>5.945312</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Reinforcement learning provides a powerful and...</td>\n",
       "      <td>SUMMARY:\\r\\nThis paper considers the Inverse R...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>reinforcement learn provide powerful general f...</td>\n",
       "      <td>summary   paper consider inverse reinforcement...</td>\n",
       "      <td>[[0.7725912]]</td>\n",
       "      <td>36.017021</td>\n",
       "      <td>6.049645</td>\n",
       "      <td>200.200000</td>\n",
       "      <td>62</td>\n",
       "      <td>77.259120</td>\n",
       "      <td>129</td>\n",
       "      <td>1005</td>\n",
       "      <td>6.049645</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>The state-of-the-art (SOTA) for mixed precisio...</td>\n",
       "      <td>This work presents a CNN training setup that u...</td>\n",
       "      <td>[[0.8586375]]</td>\n",
       "      <td>state    art  sota  mix precision train domina...</td>\n",
       "      <td>work present cnn train setup use half precisio...</td>\n",
       "      <td>[[0.83514506]]</td>\n",
       "      <td>29.793403</td>\n",
       "      <td>5.929825</td>\n",
       "      <td>196.625000</td>\n",
       "      <td>109</td>\n",
       "      <td>83.514506</td>\n",
       "      <td>229</td>\n",
       "      <td>1580</td>\n",
       "      <td>5.929825</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     Labeled text classification datasets are typic...   \n",
       "1     We propose and evaluate new techniques for com...   \n",
       "2     Deep neural networks have become the state-of-...   \n",
       "4     In the pursuit of increasingly intelligent lea...   \n",
       "5     We exploit a recently derived inversion scheme...   \n",
       "...                                                 ...   \n",
       "2743  Convolutional neural networks (CNNs) are inher...   \n",
       "2744  Recent DNN pruning algorithms have succeeded i...   \n",
       "2745  Memory Network based models have shown a remar...   \n",
       "2746  Reinforcement learning provides a powerful and...   \n",
       "2747  The state-of-the-art (SOTA) for mixed precisio...   \n",
       "\n",
       "                                                 review semantic_similarity  \\\n",
       "0     This paper proposes a language independent tex...       [[0.9102851]]   \n",
       "1     The problem considered in the paper is of comp...      [[0.88059396]]   \n",
       "2     The authors cast some of the most recent CNN d...      [[0.90264124]]   \n",
       "4     The paper presents a new policy gradient techn...      [[0.85638505]]   \n",
       "5     In summary, the paper is based on a recent wor...           999999999   \n",
       "...                                                 ...                 ...   \n",
       "2743  This paper presents a new convolutional networ...           999999999   \n",
       "2744  This paper casts the pruning optimization prob...      [[0.87137043]]   \n",
       "2745  The paper proposes to address the quadratic me...       [[0.8944535]]   \n",
       "2746  SUMMARY:\\r\\nThis paper considers the Inverse R...           999999999   \n",
       "2747  This work presents a CNN training setup that u...       [[0.8586375]]   \n",
       "\n",
       "                                         abstract_clean  \\\n",
       "0     labeled text classification dataset typically ...   \n",
       "1     propose evaluate new technique compress speed ...   \n",
       "2     deep neural network state    art model numerou...   \n",
       "4     pursuit increasingly intelligent learn system ...   \n",
       "5     exploit recently derive inversion scheme arbit...   \n",
       "...                                                 ...   \n",
       "2743  convolutional neural network  cnns  inherently...   \n",
       "2744  recent dnn prune algorithm succeed reduce numb...   \n",
       "2745  memory network base model remarkable progress ...   \n",
       "2746  reinforcement learn provide powerful general f...   \n",
       "2747  state    art  sota  mix precision train domina...   \n",
       "\n",
       "                                           review_clean  \\\n",
       "0     paper propose language independent text encode...   \n",
       "1     problem consider paper compress large network ...   \n",
       "2     author cast recent cnn design approximate solu...   \n",
       "4     paper present new policy gradient technique le...   \n",
       "5     summary  paper base recent work balestriero  b...   \n",
       "...                                                 ...   \n",
       "2743  paper present new convolutional network archit...   \n",
       "2744  paper cast prune optimization problem nettrim ...   \n",
       "2745  paper propose address quadratic memory  time r...   \n",
       "2746  summary   paper consider inverse reinforcement...   \n",
       "2747  work present cnn train setup use half precisio...   \n",
       "\n",
       "     semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                        [[0.8852489]]            13.585965          5.086207   \n",
       "1                        [[0.8604121]]            31.352227          5.517730   \n",
       "2                       [[0.88722855]]            20.851064          6.115942   \n",
       "4                        [[0.7674166]]            31.431151          5.743455   \n",
       "5                        [[0.7782445]]            37.320388          6.070588   \n",
       "...                                ...                  ...               ...   \n",
       "2743                     [[0.7938315]]            17.384127          6.038760   \n",
       "2744                    [[0.83231395]]            43.742574          5.720588   \n",
       "2745                    [[0.88223743]]            25.137931          5.945312   \n",
       "2746                     [[0.7725912]]            36.017021          6.049645   \n",
       "2747                    [[0.83514506]]            29.793403          5.929825   \n",
       "\n",
       "      avg_sen_len_abs  freq_words_gt_sen_len_abs  similarity_score  word_cnt  \\\n",
       "0          116.888889                         63         88.524890       176   \n",
       "1          182.800000                         65         86.041210       141   \n",
       "2          139.285714                        124         88.722855       276   \n",
       "4          142.444444                         97         76.741660       194   \n",
       "5          119.800000                         42         77.824450        86   \n",
       "...               ...                        ...               ...       ...   \n",
       "2743       112.500000                         53         79.383150       129   \n",
       "2744       151.333333                         68         83.231395       136   \n",
       "2745       111.500000                         71         88.223743       127   \n",
       "2746       200.200000                         62         77.259120       129   \n",
       "2747       196.625000                        109         83.514506       229   \n",
       "\n",
       "      char_len  avg_wrd_length  stopwords_cnt  num_len  \n",
       "0         1060        5.086207             69        0  \n",
       "1          918        5.517730             44        0  \n",
       "2         1963        6.115942             97        0  \n",
       "4         1290        5.743455             74        0  \n",
       "5          603        6.070588             28        0  \n",
       "...        ...             ...            ...      ...  \n",
       "2743       907        6.038760             44        0  \n",
       "2744       913        5.720588             49        0  \n",
       "2745       899        5.945312             37        0  \n",
       "2746      1005        6.049645             48        0  \n",
       "2747      1580        5.929825             70        0  \n",
       "\n",
       "[2651 rows x 16 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.word_count(data,\"abstract\",\"word_cnt\")\n",
    "tf.char_count(data,\"abstract\",\"char_len\")\n",
    "tf.avg_word_length(data,\"abstract\",\"avg_wrd_length\")\n",
    "tf.stopwords_count(data,\"abstract\",\"stopwords_cnt\")\n",
    "tf.numerics_count(data,\"abstract\",\"num_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "df = data[columns].copy()\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#df = pd.DataFrame(x_scaled)\n",
    "df=pd.DataFrame(x_scaled, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.152054</td>\n",
       "      <td>0.339162</td>\n",
       "      <td>0.446057</td>\n",
       "      <td>0.315436</td>\n",
       "      <td>0.674371</td>\n",
       "      <td>0.353116</td>\n",
       "      <td>0.368834</td>\n",
       "      <td>0.339162</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.072591</td>\n",
       "      <td>0.227040</td>\n",
       "      <td>0.351423</td>\n",
       "      <td>0.402685</td>\n",
       "      <td>0.469683</td>\n",
       "      <td>0.344214</td>\n",
       "      <td>0.381282</td>\n",
       "      <td>0.227040</td>\n",
       "      <td>0.365672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.182241</td>\n",
       "      <td>0.270005</td>\n",
       "      <td>0.447777</td>\n",
       "      <td>0.543624</td>\n",
       "      <td>0.882892</td>\n",
       "      <td>0.528190</td>\n",
       "      <td>0.528354</td>\n",
       "      <td>0.270005</td>\n",
       "      <td>0.425373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.679690</td>\n",
       "      <td>0.577372</td>\n",
       "      <td>0.565251</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.737347</td>\n",
       "      <td>0.216617</td>\n",
       "      <td>0.258645</td>\n",
       "      <td>0.577372</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.482065</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.358191</td>\n",
       "      <td>0.362416</td>\n",
       "      <td>0.827574</td>\n",
       "      <td>0.290801</td>\n",
       "      <td>0.315814</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>0.261194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "14             0.152054          0.339162         0.446057   \n",
       "15             0.072591          0.227040         0.351423   \n",
       "18             0.182241          0.270005         0.447777   \n",
       "20             0.679690          0.577372         0.565251   \n",
       "25             0.482065          0.409590         0.358191   \n",
       "\n",
       "    freq_words_gt_sen_len_abs  similarity_score  word_cnt  char_len  \\\n",
       "14                   0.315436          0.674371  0.353116  0.368834   \n",
       "15                   0.402685          0.469683  0.344214  0.381282   \n",
       "18                   0.543624          0.882892  0.528190  0.528354   \n",
       "20                   0.234899          0.737347  0.216617  0.258645   \n",
       "25                   0.362416          0.827574  0.290801  0.315814   \n",
       "\n",
       "    avg_wrd_length  stopwords_cnt  num_len  \n",
       "14        0.339162       0.343284      0.0  \n",
       "15        0.227040       0.365672      0.0  \n",
       "18        0.270005       0.425373      0.0  \n",
       "20        0.577372       0.186567      0.0  \n",
       "25        0.409590       0.261194      0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = data.sample(frac=0.8, random_state = 1)\n",
    "test_set = data.loc[~data.index.isin(training_set.index)]\n",
    "test_set.head()\n",
    "\n",
    "training_set = df.sample(frac=0.8, random_state = 1)\n",
    "test_set = df.loc[~df.index.isin(training_set.index)]\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['yules_i_measure_abs', 'avg_word_len_abs',\n",
    "       'avg_sen_len_abs', 'freq_words_gt_sen_len_abs']\n",
    "\n",
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "\n",
    "\n",
    "# training_data = training_set.as_matrix(columns = data_columns)\n",
    "training_data = training_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(training_data)\n",
    "training_data[nan_locs] = 0\n",
    "\n",
    "training_target = training_set['similarity_score'].values\n",
    "nan_locs = np.isnan(training_target)\n",
    "training_target[nan_locs] = 0\n",
    "\n",
    "test_data = test_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(test_data)\n",
    "test_data[nan_locs] = 0\n",
    "\n",
    "test_target = test_set['similarity_score'].values\n",
    "nan_locs = np.isnan(test_target)\n",
    "test_target[nan_locs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.12011333679347957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor()\n",
    "reg.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(reg.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.1448915398016214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(svr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.7980933809428177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(kr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.07324147924227065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(dt.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.08143122239111589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 2000)\n",
    "rf.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(rf.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.1564466776096709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(gb.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.13190546464869635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pr = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "              ('linear', LinearRegression(fit_intercept = False))])\n",
    "\n",
    "pr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(pr.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0008193996197520814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lm = Lasso()\n",
    "lm.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(lm.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0008193996197520814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(en.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0008193996197520814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "ll = LassoLars()\n",
    "ll.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(ll.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.11388019430554819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(br.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.027637723225301447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(sgd.score(test_data, test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
