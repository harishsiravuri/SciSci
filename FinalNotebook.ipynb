{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textstat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-4ce6c435dcad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextstat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextstatistics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0measy_word_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegacy_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textstat'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import textfeatures as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "# polysemy for the content words\n",
    "def polysemy(group):\n",
    "    stop = stopwords.words('english')\n",
    "    str1 = [i for i in group.split() if i not in stop]\n",
    "    a = list()\n",
    "    for w in str1:\n",
    "        if(len(wn.synsets(w)) > 1):\n",
    "            a.append(w)\n",
    "    return len(a)\n",
    "\n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(self, text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text)\n",
    "    return doc.sents\n",
    "# Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    "# Returns the number of sentences in the text\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words\n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables\n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    return legacy_round(FRE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "avg_sentence_length() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-ae31f379c3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"flesch_reading_ease\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"flesch_reading_ease\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflesch_reading_ease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6877\u001b[0m         )\n\u001b[1;32m-> 6878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 result = libreduction.compute_reduction(\n\u001b[0m\u001b[0;32m    296\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-123-ae31f379c3c3>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"flesch_reading_ease\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mraw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"flesch_reading_ease\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mflesch_reading_ease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-0bd5fd1c2654>\u001b[0m in \u001b[0;36mflesch_reading_ease\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 divided by number of words)\n\u001b[0;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mFRE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m206.835\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.015\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_sentence_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m           \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m84.6\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavg_syllables_per_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlegacy_round\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFRE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: avg_sentence_length() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"polysemy\"] = np.nan\n",
    "raw_data[\"polysemy\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"flesch_reading_ease\"] = np.nan\n",
    "raw_data[\"flesch_reading_ease\"] = raw_data.apply(lambda x: flesch_reading_ease(x['abstract']), axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data['similarity_score'] = pd.to_numeric(data['semantic_similarity_aftercleaning'].str.replace('[','').str.replace(']',''))\n",
    "data = data[data['similarity_score'] != 999999999.0].copy()\n",
    "data['similarity_score'] = data['similarity_score'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.word_count(data,\"abstract\",\"word_cnt\")\n",
    "tf.char_count(data,\"abstract\",\"char_len\")\n",
    "tf.avg_word_length(data,\"abstract\",\"avg_wrd_length\")\n",
    "tf.stopwords_count(data,\"abstract\",\"stopwords_cnt\")\n",
    "tf.numerics_count(data,\"abstract\",\"num_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['flesch_reading_ease'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-140ab359ac08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n\u001b[0;32m      2\u001b[0m           'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len', 'polysemy', 'flesch_reading_ease']\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;31m#returns a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmin_max_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m         self._validate_read_indexer(\n\u001b[0m\u001b[0;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1646\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['flesch_reading_ease'] not in index\""
     ]
    }
   ],
   "source": [
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len', 'polysemy', 'flesch_reading_ease']\n",
    "df = data[columns].copy()\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#df = pd.DataFrame(x_scaled)\n",
    "df=pd.DataFrame(x_scaled, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "      <th>polysemy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.283638</td>\n",
       "      <td>0.359858</td>\n",
       "      <td>0.383752</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.860262</td>\n",
       "      <td>0.661721</td>\n",
       "      <td>0.695251</td>\n",
       "      <td>0.359858</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.791946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.353237</td>\n",
       "      <td>0.489666</td>\n",
       "      <td>0.305471</td>\n",
       "      <td>0.302013</td>\n",
       "      <td>0.650185</td>\n",
       "      <td>0.255193</td>\n",
       "      <td>0.284002</td>\n",
       "      <td>0.489666</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.168668</td>\n",
       "      <td>0.601191</td>\n",
       "      <td>0.473236</td>\n",
       "      <td>0.389262</td>\n",
       "      <td>0.774737</td>\n",
       "      <td>0.403561</td>\n",
       "      <td>0.467036</td>\n",
       "      <td>0.601191</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.273380</td>\n",
       "      <td>0.268456</td>\n",
       "      <td>0.723092</td>\n",
       "      <td>0.201780</td>\n",
       "      <td>0.264638</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.352939</td>\n",
       "      <td>0.684665</td>\n",
       "      <td>0.314639</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>0.793084</td>\n",
       "      <td>0.231454</td>\n",
       "      <td>0.289534</td>\n",
       "      <td>0.684665</td>\n",
       "      <td>0.171642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "14             0.283638          0.359858         0.383752   \n",
       "15             0.353237          0.489666         0.305471   \n",
       "18             0.168668          0.601191         0.473236   \n",
       "20             0.372270          0.771313         0.273380   \n",
       "25             0.352939          0.684665         0.314639   \n",
       "\n",
       "    freq_words_gt_sen_len_abs  similarity_score  word_cnt  char_len  \\\n",
       "14                   0.791946          0.860262  0.661721  0.695251   \n",
       "15                   0.302013          0.650185  0.255193  0.284002   \n",
       "18                   0.389262          0.774737  0.403561  0.467036   \n",
       "20                   0.268456          0.723092  0.201780  0.264638   \n",
       "25                   0.261745          0.793084  0.231454  0.289534   \n",
       "\n",
       "    avg_wrd_length  stopwords_cnt  num_len  polysemy  \n",
       "14        0.359858       0.492537      0.0  0.791946  \n",
       "15        0.489666       0.194030      0.0  0.302013  \n",
       "18        0.601191       0.358209      0.0  0.389262  \n",
       "20        0.771313       0.164179      0.0  0.268456  \n",
       "25        0.684665       0.171642      0.0  0.261745  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = data.sample(frac=0.8, random_state = 1)\n",
    "test_set = data.loc[~data.index.isin(training_set.index)]\n",
    "test_set.head()\n",
    "\n",
    "training_set = df.sample(frac=0.8, random_state = 1)\n",
    "test_set = df.loc[~df.index.isin(training_set.index)]\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['yules_i_measure_abs', 'avg_word_len_abs',\n",
    "       'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'polysemy']\n",
    "\n",
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len', 'polysemy']\n",
    "\n",
    "\n",
    "# training_data = training_set.as_matrix(columns = data_columns)\n",
    "training_data = training_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(training_data)\n",
    "training_data[nan_locs] = 0\n",
    "\n",
    "training_target = training_set['similarity_score'].values\n",
    "nan_locs = np.isnan(training_target)\n",
    "training_target[nan_locs] = 0\n",
    "\n",
    "test_data = test_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(test_data)\n",
    "test_data[nan_locs] = 0\n",
    "\n",
    "test_target = test_set['similarity_score'].values\n",
    "nan_locs = np.isnan(test_target)\n",
    "test_target[nan_locs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_array = training_data.copy()\n",
    "train_class_array = training_target.copy()\n",
    "test_data_array = test_data.copy()\n",
    "test_class_array = test_target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.12353661053259768\n",
      "MAE: 0.09264346028057945\n",
      "MSE: 0.013884759816458142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor()\n",
    "reg.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(reg.score(test_data, test_target)))\n",
    "pred = reg.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.14649566647001444\n",
      "MAE: 0.09078776941844129\n",
      "MSE: 0.013521046989277339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(svr.score(test_data, test_target)))\n",
    "pred = svr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.8472842498037556\n",
      "MAE: 0.13597469011441832\n",
      "MSE: 0.029264312040275083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(kr.score(test_data, test_target)))\n",
    "pred = kr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.16370423905680132\n",
      "MAE: 0.10309986957717793\n",
      "MSE: 0.018435172593479807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(dt.score(test_data, test_target)))\n",
    "pred = dt.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.05687087074336494\n",
      "MAE: 0.09482643969027843\n",
      "MSE: 0.014940865292264124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 2000)\n",
    "rf.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(rf.score(test_data, test_target)))\n",
    "pred = rf.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.12965718103564206\n",
      "MAE: 0.09017677372147236\n",
      "MSE: 0.013787798948045703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(gb.score(test_data, test_target)))\n",
    "pred = gb.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.1342978368299239\n",
      "MAE: 0.0915514181378897\n",
      "MSE: 0.013714282596000911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pr = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "              ('linear', LinearRegression(fit_intercept = False))])\n",
    "\n",
    "pr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(pr.score(test_data, test_target)))\n",
    "pred = pr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005374783233043079\n",
      "MAE: 0.09928753141715577\n",
      "MSE: 0.01585031701361265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lm = Lasso()\n",
    "lm.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(lm.score(test_data, test_target)))\n",
    "pred = lm.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005374783233043079\n",
      "MAE: 0.09928753141715577\n",
      "MSE: 0.01585031701361265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(en.score(test_data, test_target)))\n",
    "pred = en.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0005374783233043079\n",
      "MAE: 0.09928753141715577\n",
      "MSE: 0.01585031701361265\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "ll = LassoLars()\n",
    "ll.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(ll.score(test_data, test_target)))\n",
    "pred = ll.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.09164383039022828\n",
      "MAE: 0.0936266240301292\n",
      "MSE: 0.014389998937084725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(br.score(test_data, test_target)))\n",
    "pred = br.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.011293623040671719\n",
      "MAE: 0.09887368733240863\n",
      "MSE: 0.01566289104376946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(sgd.score(test_data, test_target)))\n",
    "pred = sgd.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{table}[ht]\n",
    "\\caption{Model Results} % title of Table\n",
    "\\centering % used for centering table\n",
    "\\begin{tabular}{c c c c} % centered columns (4 columns)\n",
    "\\hline\\hline %inserts double horizontal lines\n",
    "Case & Method\\#1 & Method\\#2 & Method\\#3 \\\\ [0.5ex] % inserts table\n",
    "%heading\n",
    "\\hline % inserts single horizontal line\n",
    "1 & 50 & 837 & 970 \\\\ % inserting body of the table\n",
    "2 & 47 & 877 & 230 \\\\\n",
    "3 & 31 & 25 & 415 \\\\\n",
    "4 & 35 & 144 & 2356 \\\\\n",
    "5 & 45 & 300 & 556 \\\\ [1ex] % [1ex] adds vertical space\n",
    "\\hline %inserts single line\n",
    "\\end{tabular}\n",
    "\\label{table:nonlin} % is used to refer this table in the text\n",
    "\\end{table}\n",
    "--------------------- e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Tuned Decision Tree Parameters: {'random_state': 1558, 'min_samples_split': 0.9, 'min_samples_leaf': 0.30000000000000004, 'max_features': 2, 'max_depth': 32, 'criterion': 'mse'}\n",
      "Best score is 0.06330274150685954\n",
      "Train MSE:   0.014\n",
      "Test MSE:   0.014\n",
      "Test MAE:   0.093\n",
      "R-squared  0.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist_dt = {\"max_depth\": [1, 32],\n",
    "              \"max_features\": list(range(1, 21)),\n",
    "              \"random_state\": list(range(1, 5000)),\n",
    "              \"min_samples_split\": np.linspace(0.1, 1, 10, endpoint=True),\n",
    "              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "              \"criterion\": [\"friedman_mse\", \"mse\"]}\n",
    "## Instantiate the RandomizedSearchCV object: tree_cv\n",
    "dt_cv = RandomizedSearchCV(DecisionTreeRegressor(), \\\n",
    "                           param_dist_dt, cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit it to the data\n",
    "dt_cv.fit(train_data_array, train_class_array)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(dt_cv.best_params_))\n",
    "print(\"Best score is {}\".format(dt_cv.best_score_))\n",
    "# train mse\n",
    "pred = dt_cv.predict(train_data_array)\n",
    "score = metrics.mean_squared_error(train_class_array, pred)\n",
    "print(\"Train MSE:   %0.3f\" % score)\n",
    "# test mse\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_squared_error(test_class_array, pred)\n",
    "print(\"Test MSE:   %0.3f\" % score)\n",
    "# test mae\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_absolute_error(test_class_array, pred)\n",
    "print(\"Test MAE:   %0.3f\" % score)\n",
    "# r-squared\n",
    "score = metrics.r2_score(test_class_array, pred)\n",
    "print(\"R-squared  %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-667f026603c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgdb_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Fit it to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgdb_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_class_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# Print the tuned parameters and score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tuned Gradient Boosting Parameters: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdb_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m                     % (grid_size, self.n_iter, grid_size), UserWarning)\n\u001b[0;32m    283\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0m\u001b[0;32m    285\u001b[0m                                                 random_state=rng):\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\utils\\_random.pyx\u001b[0m in \u001b[0;36msklearn.utils._random.sample_without_replacement\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"learning_rate\": [1, 0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "              \"n_estimators\": [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "              \"random_state\": list(range(1, 5000)),\n",
    "              \"max_features\": list(range(1, 9)),\n",
    "              \"max_depth\": np.linspace(1, 32, 32, endpoint=True),\n",
    "              \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True)}\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "gdb = GradientBoostingRegressor()\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "gdb_cv = RandomizedSearchCV(gdb, param_dist, cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit it to the data\n",
    "gdb_cv.fit(train_data_array, train_class_array)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Gradient Boosting Parameters: {}\".format(gdb_cv.best_params_))\n",
    "print(\"Best score is {}\".format(rf_cv.best_score_))\n",
    "# train mse\n",
    "pred = gdb_cv.predict(train_data_array)\n",
    "score = metrics.mean_squared_error(train_class_array, pred)\n",
    "print(\"Train MSE:   %0.3f\" % score)\n",
    "# test mse\n",
    "pred = gdb_cv.predict(test_data_array)\n",
    "score = metrics.mean_squared_error(test_class_array, pred)\n",
    "print(\"Test MSE:   %0.3f\" % score)\n",
    "# test mae\n",
    "pred = gdb_cv.predict(test_data_array)\n",
    "score = metrics.mean_absolute_error(test_class_array, pred)\n",
    "print(\"Test MAE:   %0.3f\" % score)\n",
    "# r-squared\n",
    "score = metrics.r2_score(test_class_array, pred)\n",
    "print(\"R-squared  %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
