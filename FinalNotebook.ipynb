{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textstat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-4ce6c435dcad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextstat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtextstatistics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0measy_word_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegacy_round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textstat'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import textfeatures as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "# polysemy for the content words\n",
    "def polysemy(group):\n",
    "    stop = stopwords.words('english')\n",
    "    str1 = [i for i in group.split() if i not in stop]\n",
    "    a = list()\n",
    "    for w in str1:\n",
    "        if(len(wn.synsets(w)) > 1):\n",
    "            a.append(w)\n",
    "    return len(a)\n",
    "\n",
    "# Returns average sentence length\n",
    "def avg_sentence_length(self, text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "# Returns the average number of syllables per\n",
    "# word in the text\n",
    "def avg_syllables_per_word(text):\n",
    "    syllable = syllables_count(text)\n",
    "    words = word_count(text)\n",
    "    ASPW = float(syllable) / float(words)\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text)\n",
    "    return doc.sents\n",
    "# Returns Number of Words in the text\n",
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    "# Returns the number of sentences in the text\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"\n",
    "        Implements Flesch Formula:\n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "        Here,\n",
    "          ASL = average sentence length (number of words\n",
    "                divided by number of sentences)\n",
    "          ASW = average word length in syllables (number of syllables\n",
    "                divided by number of words)\n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -\\\n",
    "          float(84.6 * avg_syllables_per_word(text))\n",
    "    return legacy_round(FRE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distributional Semantics Models(DSM) derive wo...</td>\n",
       "      <td>I hate to say that the current version of this...</td>\n",
       "      <td>[[0.8928093]]</td>\n",
       "      <td>distributional semantics modelsdsm  derive wor...</td>\n",
       "      <td>hate current version paper ready  poorly write...</td>\n",
       "      <td>[[0.83050203]]</td>\n",
       "      <td>19.643612</td>\n",
       "      <td>6.035088</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teaching plays a very important role in our so...</td>\n",
       "      <td>The authors define a deep learning model compo...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>teaching play important role society  spread h...</td>\n",
       "      <td>author define deep learn model compose compone...</td>\n",
       "      <td>[[0.87180996]]</td>\n",
       "      <td>22.545755</td>\n",
       "      <td>5.706349</td>\n",
       "      <td>210.250000</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep learning achieves remarkable generalizati...</td>\n",
       "      <td>The paper aims to provide a view of deep learn...</td>\n",
       "      <td>[[0.8619562]]</td>\n",
       "      <td>deep learn achieve remarkable generalization c...</td>\n",
       "      <td>paper aim provide view deep learn perspective ...</td>\n",
       "      <td>[[0.8272942]]</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>6.619048</td>\n",
       "      <td>132.333333</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Long Short-Term Memory (LSTM) is one of the mo...</td>\n",
       "      <td>This paper propose a new \"gate\" function for L...</td>\n",
       "      <td>[[0.8917985]]</td>\n",
       "      <td>long short  term memory  lstm  widely use recu...</td>\n",
       "      <td>paper propose new  gate  function lstm enable ...</td>\n",
       "      <td>[[0.8526045]]</td>\n",
       "      <td>21.028683</td>\n",
       "      <td>5.381579</td>\n",
       "      <td>180.875000</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A core aspect of human intelligence is the ab...</td>\n",
       "      <td>The authors propose a kind of framework for le...</td>\n",
       "      <td>[[0.8901408]]</td>\n",
       "      <td>core aspect human intelligence ability learn ...</td>\n",
       "      <td>author propose kind framework learn solve elem...</td>\n",
       "      <td>[[0.88526076]]</td>\n",
       "      <td>41.541958</td>\n",
       "      <td>5.962733</td>\n",
       "      <td>159.428571</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Distributional Semantics Models(DSM) derive wo...   \n",
       "1  Teaching plays a very important role in our so...   \n",
       "2  Deep learning achieves remarkable generalizati...   \n",
       "3  Long Short-Term Memory (LSTM) is one of the mo...   \n",
       "4   A core aspect of human intelligence is the ab...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  I hate to say that the current version of this...       [[0.8928093]]   \n",
       "1  The authors define a deep learning model compo...           999999999   \n",
       "2  The paper aims to provide a view of deep learn...       [[0.8619562]]   \n",
       "3  This paper propose a new \"gate\" function for L...       [[0.8917985]]   \n",
       "4  The authors propose a kind of framework for le...       [[0.8901408]]   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  distributional semantics modelsdsm  derive wor...   \n",
       "1  teaching play important role society  spread h...   \n",
       "2  deep learn achieve remarkable generalization c...   \n",
       "3  long short  term memory  lstm  widely use recu...   \n",
       "4   core aspect human intelligence ability learn ...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  hate current version paper ready  poorly write...   \n",
       "1  author define deep learn model compose compone...   \n",
       "2  paper aim provide view deep learn perspective ...   \n",
       "3  paper propose new  gate  function lstm enable ...   \n",
       "4  author propose kind framework learn solve elem...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                    [[0.83050203]]            19.643612          6.035088   \n",
       "1                    [[0.87180996]]            22.545755          5.706349   \n",
       "2                     [[0.8272942]]            46.875000          6.619048   \n",
       "3                     [[0.8526045]]            21.028683          5.381579   \n",
       "4                    [[0.88526076]]            41.541958          5.962733   \n",
       "\n",
       "   avg_sen_len_abs  freq_words_gt_sen_len_abs  polysemy  flesch_reading_ease  \n",
       "0       126.000000                        121       121                  NaN  \n",
       "1       210.250000                        112       112                  NaN  \n",
       "2       132.333333                         57        57                  NaN  \n",
       "3       180.875000                         92        92                  NaN  \n",
       "4       159.428571                         82        82                  NaN  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"polysemy\"] = np.nan\n",
    "raw_data[\"polysemy\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "# raw_data[\"flesch_reading_ease\"] = np.nan\n",
    "# raw_data[\"flesch_reading_ease\"] = raw_data.apply(lambda x: flesch_reading_ease(x['abstract']), axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data['similarity_score'] = pd.to_numeric(data['semantic_similarity_aftercleaning'].str.replace('[','').str.replace(']',''))\n",
    "data = data[data['similarity_score'] != 999999999.0].copy()\n",
    "data['similarity_score'] = data['similarity_score'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>polysemy</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distributional Semantics Models(DSM) derive wo...</td>\n",
       "      <td>I hate to say that the current version of this...</td>\n",
       "      <td>[[0.8928093]]</td>\n",
       "      <td>distributional semantics modelsdsm  derive wor...</td>\n",
       "      <td>hate current version paper ready  poorly write...</td>\n",
       "      <td>[[0.83050203]]</td>\n",
       "      <td>19.643612</td>\n",
       "      <td>6.035088</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.050203</td>\n",
       "      <td>260</td>\n",
       "      <td>2031</td>\n",
       "      <td>6.035088</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Teaching plays a very important role in our so...</td>\n",
       "      <td>The authors define a deep learning model compo...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>teaching play important role society  spread h...</td>\n",
       "      <td>author define deep learn model compose compone...</td>\n",
       "      <td>[[0.87180996]]</td>\n",
       "      <td>22.545755</td>\n",
       "      <td>5.706349</td>\n",
       "      <td>210.250000</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.180996</td>\n",
       "      <td>252</td>\n",
       "      <td>1689</td>\n",
       "      <td>5.706349</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep learning achieves remarkable generalizati...</td>\n",
       "      <td>The paper aims to provide a view of deep learn...</td>\n",
       "      <td>[[0.8619562]]</td>\n",
       "      <td>deep learn achieve remarkable generalization c...</td>\n",
       "      <td>paper aim provide view deep learn perspective ...</td>\n",
       "      <td>[[0.8272942]]</td>\n",
       "      <td>46.875000</td>\n",
       "      <td>6.619048</td>\n",
       "      <td>132.333333</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.729420</td>\n",
       "      <td>105</td>\n",
       "      <td>799</td>\n",
       "      <td>6.619048</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Long Short-Term Memory (LSTM) is one of the mo...</td>\n",
       "      <td>This paper propose a new \"gate\" function for L...</td>\n",
       "      <td>[[0.8917985]]</td>\n",
       "      <td>long short  term memory  lstm  widely use recu...</td>\n",
       "      <td>paper propose new  gate  function lstm enable ...</td>\n",
       "      <td>[[0.8526045]]</td>\n",
       "      <td>21.028683</td>\n",
       "      <td>5.381579</td>\n",
       "      <td>180.875000</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.260450</td>\n",
       "      <td>228</td>\n",
       "      <td>1454</td>\n",
       "      <td>5.381579</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A core aspect of human intelligence is the ab...</td>\n",
       "      <td>The authors propose a kind of framework for le...</td>\n",
       "      <td>[[0.8901408]]</td>\n",
       "      <td>core aspect human intelligence ability learn ...</td>\n",
       "      <td>author propose kind framework learn solve elem...</td>\n",
       "      <td>[[0.88526076]]</td>\n",
       "      <td>41.541958</td>\n",
       "      <td>5.962733</td>\n",
       "      <td>159.428571</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.526076</td>\n",
       "      <td>163</td>\n",
       "      <td>1122</td>\n",
       "      <td>5.962733</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>We examine how learning from unaligned data ca...</td>\n",
       "      <td>This work propose a generative model for unsup...</td>\n",
       "      <td>[[0.88786817]]</td>\n",
       "      <td>examine learn unaligned datum improve datum ef...</td>\n",
       "      <td>work propose generative model unsupervised lea...</td>\n",
       "      <td>[[0.83547205]]</td>\n",
       "      <td>30.311429</td>\n",
       "      <td>5.858824</td>\n",
       "      <td>165.571429</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.547205</td>\n",
       "      <td>170</td>\n",
       "      <td>1165</td>\n",
       "      <td>5.858824</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>Deep Neural Networks (DNNs) have recently been...</td>\n",
       "      <td>This paper tried to analyze the subspaces of t...</td>\n",
       "      <td>[[0.82397217]]</td>\n",
       "      <td>deep neural networks  dnns  recently vulnerabl...</td>\n",
       "      <td>paper try analyze subspace adversarial example...</td>\n",
       "      <td>[[0.7635975]]</td>\n",
       "      <td>20.751118</td>\n",
       "      <td>6.188406</td>\n",
       "      <td>211.571429</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.359750</td>\n",
       "      <td>207</td>\n",
       "      <td>1487</td>\n",
       "      <td>6.188406</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>The success of Deep Learning and its potential...</td>\n",
       "      <td>The paper studies methods for verifying neural...</td>\n",
       "      <td>[[0.90048885]]</td>\n",
       "      <td>success deep learning potential use important ...</td>\n",
       "      <td>paper study method verify neural net piecewise...</td>\n",
       "      <td>[[0.8841833]]</td>\n",
       "      <td>25.371742</td>\n",
       "      <td>5.493506</td>\n",
       "      <td>188.750000</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.418330</td>\n",
       "      <td>213</td>\n",
       "      <td>1517</td>\n",
       "      <td>5.493506</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>We introduce a new dataset of logical entailme...</td>\n",
       "      <td>This is a wonderful and a self-contained paper...</td>\n",
       "      <td>[[0.83777016]]</td>\n",
       "      <td>introduce new dataset logical entailments purp...</td>\n",
       "      <td>wonderful self  contain paper  fact  introduce...</td>\n",
       "      <td>[[0.83051616]]</td>\n",
       "      <td>28.195122</td>\n",
       "      <td>6.058824</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.051616</td>\n",
       "      <td>102</td>\n",
       "      <td>719</td>\n",
       "      <td>6.058824</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>Long Short-Term Memory (LSTM) units have the a...</td>\n",
       "      <td>Summary: This paper introduces a model that co...</td>\n",
       "      <td>[[0.8302729]]</td>\n",
       "      <td>long short  term memory  lstm  unit ability me...</td>\n",
       "      <td>summary  paper introduce model combine rotatio...</td>\n",
       "      <td>[[0.77381945]]</td>\n",
       "      <td>38.111111</td>\n",
       "      <td>5.451613</td>\n",
       "      <td>132.333333</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.381945</td>\n",
       "      <td>62</td>\n",
       "      <td>399</td>\n",
       "      <td>5.451613</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     Distributional Semantics Models(DSM) derive wo...   \n",
       "1     Teaching plays a very important role in our so...   \n",
       "2     Deep learning achieves remarkable generalizati...   \n",
       "3     Long Short-Term Memory (LSTM) is one of the mo...   \n",
       "4      A core aspect of human intelligence is the ab...   \n",
       "...                                                 ...   \n",
       "2743  We examine how learning from unaligned data ca...   \n",
       "2744  Deep Neural Networks (DNNs) have recently been...   \n",
       "2745  The success of Deep Learning and its potential...   \n",
       "2746  We introduce a new dataset of logical entailme...   \n",
       "2747  Long Short-Term Memory (LSTM) units have the a...   \n",
       "\n",
       "                                                 review semantic_similarity  \\\n",
       "0     I hate to say that the current version of this...       [[0.8928093]]   \n",
       "1     The authors define a deep learning model compo...           999999999   \n",
       "2     The paper aims to provide a view of deep learn...       [[0.8619562]]   \n",
       "3     This paper propose a new \"gate\" function for L...       [[0.8917985]]   \n",
       "4     The authors propose a kind of framework for le...       [[0.8901408]]   \n",
       "...                                                 ...                 ...   \n",
       "2743  This work propose a generative model for unsup...      [[0.88786817]]   \n",
       "2744  This paper tried to analyze the subspaces of t...      [[0.82397217]]   \n",
       "2745  The paper studies methods for verifying neural...      [[0.90048885]]   \n",
       "2746  This is a wonderful and a self-contained paper...      [[0.83777016]]   \n",
       "2747  Summary: This paper introduces a model that co...       [[0.8302729]]   \n",
       "\n",
       "                                         abstract_clean  \\\n",
       "0     distributional semantics modelsdsm  derive wor...   \n",
       "1     teaching play important role society  spread h...   \n",
       "2     deep learn achieve remarkable generalization c...   \n",
       "3     long short  term memory  lstm  widely use recu...   \n",
       "4      core aspect human intelligence ability learn ...   \n",
       "...                                                 ...   \n",
       "2743  examine learn unaligned datum improve datum ef...   \n",
       "2744  deep neural networks  dnns  recently vulnerabl...   \n",
       "2745  success deep learning potential use important ...   \n",
       "2746  introduce new dataset logical entailments purp...   \n",
       "2747  long short  term memory  lstm  unit ability me...   \n",
       "\n",
       "                                           review_clean  \\\n",
       "0     hate current version paper ready  poorly write...   \n",
       "1     author define deep learn model compose compone...   \n",
       "2     paper aim provide view deep learn perspective ...   \n",
       "3     paper propose new  gate  function lstm enable ...   \n",
       "4     author propose kind framework learn solve elem...   \n",
       "...                                                 ...   \n",
       "2743  work propose generative model unsupervised lea...   \n",
       "2744  paper try analyze subspace adversarial example...   \n",
       "2745  paper study method verify neural net piecewise...   \n",
       "2746  wonderful self  contain paper  fact  introduce...   \n",
       "2747  summary  paper introduce model combine rotatio...   \n",
       "\n",
       "     semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                       [[0.83050203]]            19.643612          6.035088   \n",
       "1                       [[0.87180996]]            22.545755          5.706349   \n",
       "2                        [[0.8272942]]            46.875000          6.619048   \n",
       "3                        [[0.8526045]]            21.028683          5.381579   \n",
       "4                       [[0.88526076]]            41.541958          5.962733   \n",
       "...                                ...                  ...               ...   \n",
       "2743                    [[0.83547205]]            30.311429          5.858824   \n",
       "2744                     [[0.7635975]]            20.751118          6.188406   \n",
       "2745                     [[0.8841833]]            25.371742          5.493506   \n",
       "2746                    [[0.83051616]]            28.195122          6.058824   \n",
       "2747                    [[0.77381945]]            38.111111          5.451613   \n",
       "\n",
       "      avg_sen_len_abs  freq_words_gt_sen_len_abs  polysemy  \\\n",
       "0          126.000000                        121       121   \n",
       "1          210.250000                        112       112   \n",
       "2          132.333333                         57        57   \n",
       "3          180.875000                         92        92   \n",
       "4          159.428571                         82        82   \n",
       "...               ...                        ...       ...   \n",
       "2743       165.571429                         75        75   \n",
       "2744       211.571429                         87        87   \n",
       "2745       188.750000                        100       100   \n",
       "2746       239.000000                         44        44   \n",
       "2747       132.333333                         26        26   \n",
       "\n",
       "      flesch_reading_ease  similarity_score  word_cnt  char_len  \\\n",
       "0                     NaN         83.050203       260      2031   \n",
       "1                     NaN         87.180996       252      1689   \n",
       "2                     NaN         82.729420       105       799   \n",
       "3                     NaN         85.260450       228      1454   \n",
       "4                     NaN         88.526076       163      1122   \n",
       "...                   ...               ...       ...       ...   \n",
       "2743                  NaN         83.547205       170      1165   \n",
       "2744                  NaN         76.359750       207      1487   \n",
       "2745                  NaN         88.418330       213      1517   \n",
       "2746                  NaN         83.051616       102       719   \n",
       "2747                  NaN         77.381945        62       399   \n",
       "\n",
       "      avg_wrd_length  stopwords_cnt  num_len  \n",
       "0           6.035088             88        0  \n",
       "1           5.706349             90        0  \n",
       "2           6.619048             27        0  \n",
       "3           5.381579             87        1  \n",
       "4           5.962733             49        0  \n",
       "...              ...            ...      ...  \n",
       "2743        5.858824             57        0  \n",
       "2744        6.188406             75        0  \n",
       "2745        5.493506             94        0  \n",
       "2746        6.058824             38        0  \n",
       "2747        5.451613             21        0  \n",
       "\n",
       "[2651 rows x 18 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.word_count(data,\"abstract\",\"word_cnt\")\n",
    "tf.char_count(data,\"abstract\",\"char_len\")\n",
    "tf.avg_word_length(data,\"abstract\",\"avg_wrd_length\")\n",
    "tf.stopwords_count(data,\"abstract\",\"stopwords_cnt\")\n",
    "tf.numerics_count(data,\"abstract\",\"num_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len', 'polysemy']\n",
    "df = data[columns].copy()\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#df = pd.DataFrame(x_scaled)\n",
    "df=pd.DataFrame(x_scaled, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "      <th>polysemy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.238270</td>\n",
       "      <td>0.378058</td>\n",
       "      <td>0.434597</td>\n",
       "      <td>0.449664</td>\n",
       "      <td>0.901918</td>\n",
       "      <td>0.415430</td>\n",
       "      <td>0.439834</td>\n",
       "      <td>0.378058</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.449664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.181193</td>\n",
       "      <td>0.529975</td>\n",
       "      <td>0.571472</td>\n",
       "      <td>0.483221</td>\n",
       "      <td>0.798186</td>\n",
       "      <td>0.412463</td>\n",
       "      <td>0.536192</td>\n",
       "      <td>0.529975</td>\n",
       "      <td>0.350746</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.483221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.421369</td>\n",
       "      <td>0.530478</td>\n",
       "      <td>0.228301</td>\n",
       "      <td>0.342282</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>0.359050</td>\n",
       "      <td>0.408944</td>\n",
       "      <td>0.530478</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.342282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.234547</td>\n",
       "      <td>0.423638</td>\n",
       "      <td>0.681579</td>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.809670</td>\n",
       "      <td>0.670623</td>\n",
       "      <td>0.716459</td>\n",
       "      <td>0.423638</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.731544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.115274</td>\n",
       "      <td>0.809900</td>\n",
       "      <td>0.608802</td>\n",
       "      <td>0.449664</td>\n",
       "      <td>0.650885</td>\n",
       "      <td>0.367953</td>\n",
       "      <td>0.467036</td>\n",
       "      <td>0.809900</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.449664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "14             0.238270          0.378058         0.434597   \n",
       "15             0.181193          0.529975         0.571472   \n",
       "18             0.421369          0.530478         0.228301   \n",
       "20             0.234547          0.423638         0.681579   \n",
       "25             0.115274          0.809900         0.608802   \n",
       "\n",
       "    freq_words_gt_sen_len_abs  similarity_score  word_cnt  char_len  \\\n",
       "14                   0.449664          0.901918  0.415430  0.439834   \n",
       "15                   0.483221          0.798186  0.412463  0.536192   \n",
       "18                   0.342282          0.647917  0.359050  0.408944   \n",
       "20                   0.731544          0.809670  0.670623  0.716459   \n",
       "25                   0.449664          0.650885  0.367953  0.467036   \n",
       "\n",
       "    avg_wrd_length  stopwords_cnt  num_len  polysemy  \n",
       "14        0.378058       0.358209     0.00  0.449664  \n",
       "15        0.529975       0.350746     0.25  0.483221  \n",
       "18        0.530478       0.313433     0.00  0.342282  \n",
       "20        0.423638       0.529851     0.00  0.731544  \n",
       "25        0.809900       0.298507     0.00  0.449664  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = data.sample(frac=0.8, random_state = 1)\n",
    "test_set = data.loc[~data.index.isin(training_set.index)]\n",
    "test_set.head()\n",
    "\n",
    "training_set = df.sample(frac=0.8, random_state = 1)\n",
    "test_set = df.loc[~df.index.isin(training_set.index)]\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['yules_i_measure_abs', 'avg_word_len_abs',\n",
    "       'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'polysemy']\n",
    "\n",
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len', 'polysemy']\n",
    "\n",
    "\n",
    "# training_data = training_set.as_matrix(columns = data_columns)\n",
    "training_data = training_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(training_data)\n",
    "training_data[nan_locs] = 0\n",
    "\n",
    "training_target = training_set['similarity_score'].values\n",
    "nan_locs = np.isnan(training_target)\n",
    "training_target[nan_locs] = 0\n",
    "\n",
    "test_data = test_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(test_data)\n",
    "test_data[nan_locs] = 0\n",
    "\n",
    "test_target = test_set['similarity_score'].values\n",
    "nan_locs = np.isnan(test_target)\n",
    "test_target[nan_locs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_array = training_data.copy()\n",
    "train_class_array = training_target.copy()\n",
    "test_data_array = test_data.copy()\n",
    "test_class_array = test_target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.08545539471761376\n",
      "MAE: 0.0902223422989583\n",
      "MSE: 0.013266340136723107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor()\n",
    "reg.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(reg.score(test_data, test_target)))\n",
    "pred = reg.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.06118514264576225\n",
      "MAE: 0.09118600136808633\n",
      "MSE: 0.013618403248056831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(svr.score(test_data, test_target)))\n",
    "pred = svr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -1.070930251245128\n",
      "MAE: 0.13903085048855154\n",
      "MSE: 0.03004081479870978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(kr.score(test_data, test_target)))\n",
    "pred = kr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.28144660267259747\n",
      "MAE: 0.10402708935663606\n",
      "MSE: 0.018588602895813683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(dt.score(test_data, test_target)))\n",
    "pred = dt.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.05002391424552077\n",
      "MAE: 0.09518650660501492\n",
      "MSE: 0.015231596488148614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 2000)\n",
    "rf.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(rf.score(test_data, test_target)))\n",
    "pred = rf.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.10565330162846309\n",
      "MAE: 0.08921293779777568\n",
      "MSE: 0.012973350268780623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(gb.score(test_data, test_target)))\n",
    "pred = gb.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.0834575754813025\n",
      "MAE: 0.09000351718415334\n",
      "MSE: 0.013295320406649262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pr = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "              ('linear', LinearRegression(fit_intercept = False))])\n",
    "\n",
    "pr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(pr.score(test_data, test_target)))\n",
    "pred = pr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0006330899803945744\n",
      "MAE: 0.09644034345586966\n",
      "MSE: 0.014515135562623868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lm = Lasso()\n",
    "lm.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(lm.score(test_data, test_target)))\n",
    "pred = lm.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0006330899803945744\n",
      "MAE: 0.09644034345586966\n",
      "MSE: 0.014515135562623868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(en.score(test_data, test_target)))\n",
    "pred = en.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.0006330899803945744\n",
      "MAE: 0.09644034345586966\n",
      "MSE: 0.014515135562623868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "ll = LassoLars()\n",
    "ll.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(ll.score(test_data, test_target)))\n",
    "pred = ll.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.056345437170144175\n",
      "MAE: 0.09239987453181636\n",
      "MSE: 0.01368860778333074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(br.score(test_data, test_target)))\n",
    "pred = br.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.04704854412818005\n",
      "MAE: 0.09822602704467151\n",
      "MSE: 0.015188435912074692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(sgd.score(test_data, test_target)))\n",
    "pred = sgd.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{table}[ht]\n",
    "\\caption{Model Results} % title of Table\n",
    "\\centering % used for centering table\n",
    "\\begin{tabular}{c c c c} % centered columns (4 columns)\n",
    "\\hline\\hline %inserts double horizontal lines\n",
    "Case & Method\\#1 & Method\\#2 & Method\\#3 \\\\ [0.5ex] % inserts table\n",
    "%heading\n",
    "\\hline % inserts single horizontal line\n",
    "1 & 50 & 837 & 970 \\\\ % inserting body of the table\n",
    "2 & 47 & 877 & 230 \\\\\n",
    "3 & 31 & 25 & 415 \\\\\n",
    "4 & 35 & 144 & 2356 \\\\\n",
    "5 & 45 & 300 & 556 \\\\ [1ex] % [1ex] adds vertical space\n",
    "\\hline %inserts single line\n",
    "\\end{tabular}\n",
    "\\label{table:nonlin} % is used to refer this table in the text\n",
    "\\end{table}\n",
    "--------------------- e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'random_state': 1124, 'min_samples_split': 0.1, 'min_samples_leaf': 0.2, 'max_features': 4, 'max_depth': 32, 'criterion': 'friedman_mse'}\n",
      "Best score is 0.08582546032863245\n",
      "Train MSE:   0.013\n",
      "Test MSE:   0.013\n",
      "Test MAE:   0.091\n",
      "R-squared  0.095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist_dt = {\"max_depth\": [1, 32],\n",
    "              \"max_features\": list(range(1, 21)),\n",
    "              \"random_state\": list(range(1, 5000)),\n",
    "              \"min_samples_split\": np.linspace(0.1, 1, 10, endpoint=True),\n",
    "              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "              \"criterion\": [\"friedman_mse\", \"mse\"]}\n",
    "## Instantiate the RandomizedSearchCV object: tree_cv\n",
    "dt_cv = RandomizedSearchCV(DecisionTreeRegressor(), \\\n",
    "                           param_dist_dt, cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit it to the data\n",
    "dt_cv.fit(train_data_array, train_class_array)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(dt_cv.best_params_))\n",
    "print(\"Best score is {}\".format(dt_cv.best_score_))\n",
    "# train mse\n",
    "pred = dt_cv.predict(train_data_array)\n",
    "score = metrics.mean_squared_error(train_class_array, pred)\n",
    "print(\"Train MSE:   %0.3f\" % score)\n",
    "# test mse\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_squared_error(test_class_array, pred)\n",
    "print(\"Test MSE:   %0.3f\" % score)\n",
    "# test mae\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_absolute_error(test_class_array, pred)\n",
    "print(\"Test MAE:   %0.3f\" % score)\n",
    "# r-squared\n",
    "score = metrics.r2_score(test_class_array, pred)\n",
    "print(\"R-squared  %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-667f026603c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgdb_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Fit it to the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgdb_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_class_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# Print the tuned parameters and score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tuned Gradient Boosting Parameters: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgdb_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1527\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1529\u001b[1;33m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[0;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             random_state=self.random_state))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[0mcandidate_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m                     % (grid_size, self.n_iter, grid_size), UserWarning)\n\u001b[0;32m    283\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m             for i in sample_without_replacement(grid_size, n_iter,\n\u001b[0m\u001b[0;32m    285\u001b[0m                                                 random_state=rng):\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\utils\\_random.pyx\u001b[0m in \u001b[0;36msklearn.utils._random.sample_without_replacement\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"learning_rate\": [1, 0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "              \"n_estimators\": [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "              \"random_state\": list(range(1, 5000)),\n",
    "              \"max_features\": list(range(1, 9)),\n",
    "              \"max_depth\": np.linspace(1, 32, 32, endpoint=True),\n",
    "              \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True)}\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "gdb = GradientBoostingRegressor()\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "gdb_cv = RandomizedSearchCV(gdb, param_dist, cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit it to the data\n",
    "gdb_cv.fit(train_data_array, train_class_array)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Gradient Boosting Parameters: {}\".format(gdb_cv.best_params_))\n",
    "print(\"Best score is {}\".format(rf_cv.best_score_))\n",
    "# train mse\n",
    "pred = gdb_cv.predict(train_data_array)\n",
    "score = metrics.mean_squared_error(train_class_array, pred)\n",
    "print(\"Train MSE:   %0.3f\" % score)\n",
    "# test mse\n",
    "pred = gdb_cv.predict(test_data_array)\n",
    "score = metrics.mean_squared_error(test_class_array, pred)\n",
    "print(\"Test MSE:   %0.3f\" % score)\n",
    "# test mae\n",
    "pred = gdb_cv.predict(test_data_array)\n",
    "score = metrics.mean_absolute_error(test_class_array, pred)\n",
    "print(\"Test MAE:   %0.3f\" % score)\n",
    "# r-squared\n",
    "score = metrics.r2_score(test_class_array, pred)\n",
    "print(\"R-squared  %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
