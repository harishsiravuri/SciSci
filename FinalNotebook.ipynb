{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk import word_tokenize as tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from nltk.collocations import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import textfeatures as tf\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"scores.csv\")\n",
    "# raw_data = raw_data.sample(frac=0.01).reset_index(drop=True)\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We propose a new, multi-component energy funct...</td>\n",
       "      <td>Quick summary:\\r\\nThis paper proposes an energ...</td>\n",
       "      <td>[[0.86421597]]</td>\n",
       "      <td>propose new  multi  component energy function ...</td>\n",
       "      <td>quick summary   paper propose energy base form...</td>\n",
       "      <td>[[0.84397495]]</td>\n",
       "      <td>30.941406</td>\n",
       "      <td>5.766423</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised learning depends on annotated examp...</td>\n",
       "      <td>This paper focuses on the learning-from-crowds...</td>\n",
       "      <td>[[0.9104235]]</td>\n",
       "      <td>supervised learn depend annotate example  grin...</td>\n",
       "      <td>paper focus learn   crowd problem   noisy labe...</td>\n",
       "      <td>[[0.89139616]]</td>\n",
       "      <td>30.090020</td>\n",
       "      <td>5.872038</td>\n",
       "      <td>119.916667</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labeled text classification datasets are typic...</td>\n",
       "      <td>The draft proposes an approach to cross-lingua...</td>\n",
       "      <td>[[0.8746078]]</td>\n",
       "      <td>labeled text classification dataset typically ...</td>\n",
       "      <td>draft propose approach cross  lingual text cla...</td>\n",
       "      <td>[[0.85585344]]</td>\n",
       "      <td>13.585965</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>116.888889</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We propose a novel way of reducing the number ...</td>\n",
       "      <td>This paper examines sparse connection patterns...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>propose novel way reduce numb parameter storag...</td>\n",
       "      <td>paper examine sparse connection pattern upper ...</td>\n",
       "      <td>[[0.79835767]]</td>\n",
       "      <td>23.918644</td>\n",
       "      <td>5.618321</td>\n",
       "      <td>172.400000</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actor-critic methods solve reinforcement learn...</td>\n",
       "      <td>The paper presents a clever trick for updating...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>actor  critic method solve reinforcement learn...</td>\n",
       "      <td>paper present clever trick update actor actor ...</td>\n",
       "      <td>[[0.8510821]]</td>\n",
       "      <td>13.005803</td>\n",
       "      <td>5.389937</td>\n",
       "      <td>112.111111</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  We propose a new, multi-component energy funct...   \n",
       "1  Supervised learning depends on annotated examp...   \n",
       "2  Labeled text classification datasets are typic...   \n",
       "3  We propose a novel way of reducing the number ...   \n",
       "4  Actor-critic methods solve reinforcement learn...   \n",
       "\n",
       "                                              review semantic_similarity  \\\n",
       "0  Quick summary:\\r\\nThis paper proposes an energ...      [[0.86421597]]   \n",
       "1  This paper focuses on the learning-from-crowds...       [[0.9104235]]   \n",
       "2  The draft proposes an approach to cross-lingua...       [[0.8746078]]   \n",
       "3  This paper examines sparse connection patterns...           999999999   \n",
       "4  The paper presents a clever trick for updating...           999999999   \n",
       "\n",
       "                                      abstract_clean  \\\n",
       "0  propose new  multi  component energy function ...   \n",
       "1  supervised learn depend annotate example  grin...   \n",
       "2  labeled text classification dataset typically ...   \n",
       "3  propose novel way reduce numb parameter storag...   \n",
       "4  actor  critic method solve reinforcement learn...   \n",
       "\n",
       "                                        review_clean  \\\n",
       "0  quick summary   paper propose energy base form...   \n",
       "1  paper focus learn   crowd problem   noisy labe...   \n",
       "2  draft propose approach cross  lingual text cla...   \n",
       "3  paper examine sparse connection pattern upper ...   \n",
       "4  paper present clever trick update actor actor ...   \n",
       "\n",
       "  semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                    [[0.84397495]]            30.941406          5.766423   \n",
       "1                    [[0.89139616]]            30.090020          5.872038   \n",
       "2                    [[0.85585344]]            13.585965          5.086207   \n",
       "3                    [[0.79835767]]            23.918644          5.618321   \n",
       "4                     [[0.8510821]]            13.005803          5.389937   \n",
       "\n",
       "   avg_sen_len_abs  freq_words_gt_sen_len_abs  \n",
       "0       153.500000                         63  \n",
       "1       119.916667                        104  \n",
       "2       116.888889                         63  \n",
       "3       172.400000                         61  \n",
       "4       112.111111                         71  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"yules_i_measure_abs\"] = np.nan\n",
    "raw_data[\"yules_i_measure_abs\"] = raw_data.apply(lambda x: compute_yules_i_for_text(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_word_len_abs\"] = np.nan\n",
    "raw_data[\"avg_word_len_abs\"] = raw_data.apply(lambda x: compute_average_word_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"avg_sen_len_abs\"] = np.nan\n",
    "raw_data[\"avg_sen_len_abs\"] = raw_data.apply(lambda x: compute_average_sentence_length(x['abstract']), axis=1)\n",
    "\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = np.nan\n",
    "raw_data[\"freq_words_gt_sen_len_abs\"] = raw_data.apply(lambda x: freq_of_words_great_sent_len(x['abstract']), axis=1)\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()\n",
    "data['similarity_score'] = pd.to_numeric(data['semantic_similarity_aftercleaning'].str.replace('[','').str.replace(']',''))\n",
    "data = data[data['similarity_score'] != 999999999.0].copy()\n",
    "data['similarity_score'] = data['similarity_score'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>review</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>abstract_clean</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>semantic_similarity_aftercleaning</th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We propose a new, multi-component energy funct...</td>\n",
       "      <td>Quick summary:\\r\\nThis paper proposes an energ...</td>\n",
       "      <td>[[0.86421597]]</td>\n",
       "      <td>propose new  multi  component energy function ...</td>\n",
       "      <td>quick summary   paper propose energy base form...</td>\n",
       "      <td>[[0.84397495]]</td>\n",
       "      <td>30.941406</td>\n",
       "      <td>5.766423</td>\n",
       "      <td>153.500000</td>\n",
       "      <td>63</td>\n",
       "      <td>84.397495</td>\n",
       "      <td>137</td>\n",
       "      <td>926</td>\n",
       "      <td>5.766423</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supervised learning depends on annotated examp...</td>\n",
       "      <td>This paper focuses on the learning-from-crowds...</td>\n",
       "      <td>[[0.9104235]]</td>\n",
       "      <td>supervised learn depend annotate example  grin...</td>\n",
       "      <td>paper focus learn   crowd problem   noisy labe...</td>\n",
       "      <td>[[0.89139616]]</td>\n",
       "      <td>30.090020</td>\n",
       "      <td>5.872038</td>\n",
       "      <td>119.916667</td>\n",
       "      <td>104</td>\n",
       "      <td>89.139616</td>\n",
       "      <td>212</td>\n",
       "      <td>1450</td>\n",
       "      <td>5.872038</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Labeled text classification datasets are typic...</td>\n",
       "      <td>The draft proposes an approach to cross-lingua...</td>\n",
       "      <td>[[0.8746078]]</td>\n",
       "      <td>labeled text classification dataset typically ...</td>\n",
       "      <td>draft propose approach cross  lingual text cla...</td>\n",
       "      <td>[[0.85585344]]</td>\n",
       "      <td>13.585965</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>116.888889</td>\n",
       "      <td>63</td>\n",
       "      <td>85.585344</td>\n",
       "      <td>176</td>\n",
       "      <td>1060</td>\n",
       "      <td>5.086207</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We propose a novel way of reducing the number ...</td>\n",
       "      <td>This paper examines sparse connection patterns...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>propose novel way reduce numb parameter storag...</td>\n",
       "      <td>paper examine sparse connection pattern upper ...</td>\n",
       "      <td>[[0.79835767]]</td>\n",
       "      <td>23.918644</td>\n",
       "      <td>5.618321</td>\n",
       "      <td>172.400000</td>\n",
       "      <td>61</td>\n",
       "      <td>79.835767</td>\n",
       "      <td>131</td>\n",
       "      <td>866</td>\n",
       "      <td>5.618321</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actor-critic methods solve reinforcement learn...</td>\n",
       "      <td>The paper presents a clever trick for updating...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>actor  critic method solve reinforcement learn...</td>\n",
       "      <td>paper present clever trick update actor actor ...</td>\n",
       "      <td>[[0.8510821]]</td>\n",
       "      <td>13.005803</td>\n",
       "      <td>5.389937</td>\n",
       "      <td>112.111111</td>\n",
       "      <td>71</td>\n",
       "      <td>85.108210</td>\n",
       "      <td>159</td>\n",
       "      <td>1017</td>\n",
       "      <td>5.389937</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>Deep learning models require extensive archite...</td>\n",
       "      <td>The paper proposes an extension of the Neural ...</td>\n",
       "      <td>999999999</td>\n",
       "      <td>deep learn model require extensive architectur...</td>\n",
       "      <td>paper propose extension neural architecture se...</td>\n",
       "      <td>[[0.85404795]]</td>\n",
       "      <td>16.524831</td>\n",
       "      <td>5.812245</td>\n",
       "      <td>138.166667</td>\n",
       "      <td>112</td>\n",
       "      <td>85.404795</td>\n",
       "      <td>244</td>\n",
       "      <td>1669</td>\n",
       "      <td>5.812245</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>We consider neural network training, in applic...</td>\n",
       "      <td>The paper is well-written which makes it easy ...</td>\n",
       "      <td>[[0.89319736]]</td>\n",
       "      <td>consider neural network train  application pos...</td>\n",
       "      <td>paper good  write easy understand main  thrust...</td>\n",
       "      <td>[[0.8911872]]</td>\n",
       "      <td>11.806375</td>\n",
       "      <td>5.139013</td>\n",
       "      <td>136.600000</td>\n",
       "      <td>85</td>\n",
       "      <td>89.118720</td>\n",
       "      <td>224</td>\n",
       "      <td>1375</td>\n",
       "      <td>5.139013</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>Adversarial feature learning (AFL) is one of t...</td>\n",
       "      <td>- The authors propose the use of multiple adve...</td>\n",
       "      <td>[[0.9064467]]</td>\n",
       "      <td>adversarial feature learn  afl  promise way ex...</td>\n",
       "      <td>author propose use multiple adversary random ...</td>\n",
       "      <td>[[0.8601346]]</td>\n",
       "      <td>13.690000</td>\n",
       "      <td>5.808612</td>\n",
       "      <td>177.750000</td>\n",
       "      <td>94</td>\n",
       "      <td>86.013460</td>\n",
       "      <td>210</td>\n",
       "      <td>1429</td>\n",
       "      <td>5.808612</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>Regularization is a big issue for training dee...</td>\n",
       "      <td>the paper adapts the information bottleneck me...</td>\n",
       "      <td>[[0.89342827]]</td>\n",
       "      <td>regularization big issue train deep neural net...</td>\n",
       "      <td>paper adapt information bottleneck method prob...</td>\n",
       "      <td>[[0.7793288]]</td>\n",
       "      <td>28.543210</td>\n",
       "      <td>5.960784</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>48</td>\n",
       "      <td>77.932880</td>\n",
       "      <td>102</td>\n",
       "      <td>709</td>\n",
       "      <td>5.960784</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>We consider the problem of training generative...</td>\n",
       "      <td>The paper demonstrates the need and usage for ...</td>\n",
       "      <td>[[0.92080617]]</td>\n",
       "      <td>consider problem train generative model deep n...</td>\n",
       "      <td>paper demonstrate need usage flexible prior la...</td>\n",
       "      <td>[[0.84998953]]</td>\n",
       "      <td>27.252252</td>\n",
       "      <td>5.725000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>84.998953</td>\n",
       "      <td>78</td>\n",
       "      <td>539</td>\n",
       "      <td>5.725000</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2651 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abstract  \\\n",
       "0     We propose a new, multi-component energy funct...   \n",
       "1     Supervised learning depends on annotated examp...   \n",
       "2     Labeled text classification datasets are typic...   \n",
       "3     We propose a novel way of reducing the number ...   \n",
       "4     Actor-critic methods solve reinforcement learn...   \n",
       "...                                                 ...   \n",
       "2743  Deep learning models require extensive archite...   \n",
       "2744  We consider neural network training, in applic...   \n",
       "2745  Adversarial feature learning (AFL) is one of t...   \n",
       "2746  Regularization is a big issue for training dee...   \n",
       "2747  We consider the problem of training generative...   \n",
       "\n",
       "                                                 review semantic_similarity  \\\n",
       "0     Quick summary:\\r\\nThis paper proposes an energ...      [[0.86421597]]   \n",
       "1     This paper focuses on the learning-from-crowds...       [[0.9104235]]   \n",
       "2     The draft proposes an approach to cross-lingua...       [[0.8746078]]   \n",
       "3     This paper examines sparse connection patterns...           999999999   \n",
       "4     The paper presents a clever trick for updating...           999999999   \n",
       "...                                                 ...                 ...   \n",
       "2743  The paper proposes an extension of the Neural ...           999999999   \n",
       "2744  The paper is well-written which makes it easy ...      [[0.89319736]]   \n",
       "2745  - The authors propose the use of multiple adve...       [[0.9064467]]   \n",
       "2746  the paper adapts the information bottleneck me...      [[0.89342827]]   \n",
       "2747  The paper demonstrates the need and usage for ...      [[0.92080617]]   \n",
       "\n",
       "                                         abstract_clean  \\\n",
       "0     propose new  multi  component energy function ...   \n",
       "1     supervised learn depend annotate example  grin...   \n",
       "2     labeled text classification dataset typically ...   \n",
       "3     propose novel way reduce numb parameter storag...   \n",
       "4     actor  critic method solve reinforcement learn...   \n",
       "...                                                 ...   \n",
       "2743  deep learn model require extensive architectur...   \n",
       "2744  consider neural network train  application pos...   \n",
       "2745  adversarial feature learn  afl  promise way ex...   \n",
       "2746  regularization big issue train deep neural net...   \n",
       "2747  consider problem train generative model deep n...   \n",
       "\n",
       "                                           review_clean  \\\n",
       "0     quick summary   paper propose energy base form...   \n",
       "1     paper focus learn   crowd problem   noisy labe...   \n",
       "2     draft propose approach cross  lingual text cla...   \n",
       "3     paper examine sparse connection pattern upper ...   \n",
       "4     paper present clever trick update actor actor ...   \n",
       "...                                                 ...   \n",
       "2743  paper propose extension neural architecture se...   \n",
       "2744  paper good  write easy understand main  thrust...   \n",
       "2745   author propose use multiple adversary random ...   \n",
       "2746  paper adapt information bottleneck method prob...   \n",
       "2747  paper demonstrate need usage flexible prior la...   \n",
       "\n",
       "     semantic_similarity_aftercleaning  yules_i_measure_abs  avg_word_len_abs  \\\n",
       "0                       [[0.84397495]]            30.941406          5.766423   \n",
       "1                       [[0.89139616]]            30.090020          5.872038   \n",
       "2                       [[0.85585344]]            13.585965          5.086207   \n",
       "3                       [[0.79835767]]            23.918644          5.618321   \n",
       "4                        [[0.8510821]]            13.005803          5.389937   \n",
       "...                                ...                  ...               ...   \n",
       "2743                    [[0.85404795]]            16.524831          5.812245   \n",
       "2744                     [[0.8911872]]            11.806375          5.139013   \n",
       "2745                     [[0.8601346]]            13.690000          5.808612   \n",
       "2746                     [[0.7793288]]            28.543210          5.960784   \n",
       "2747                    [[0.84998953]]            27.252252          5.725000   \n",
       "\n",
       "      avg_sen_len_abs  freq_words_gt_sen_len_abs  similarity_score  word_cnt  \\\n",
       "0          153.500000                         63         84.397495       137   \n",
       "1          119.916667                        104         89.139616       212   \n",
       "2          116.888889                         63         85.585344       176   \n",
       "3          172.400000                         61         79.835767       131   \n",
       "4          112.111111                         71         85.108210       159   \n",
       "...               ...                        ...               ...       ...   \n",
       "2743       138.166667                        112         85.404795       244   \n",
       "2744       136.600000                         85         89.118720       224   \n",
       "2745       177.750000                         94         86.013460       210   \n",
       "2746       141.000000                         48         77.932880       102   \n",
       "2747       134.000000                         39         84.998953        78   \n",
       "\n",
       "      char_len  avg_wrd_length  stopwords_cnt  num_len  \n",
       "0          926        5.766423             49        0  \n",
       "1         1450        5.872038             63        0  \n",
       "2         1060        5.086207             69        0  \n",
       "3          866        5.618321             46        0  \n",
       "4         1017        5.389937             61        0  \n",
       "...        ...             ...            ...      ...  \n",
       "2743      1669        5.812245             77        0  \n",
       "2744      1375        5.139013             93        0  \n",
       "2745      1429        5.808612             80        0  \n",
       "2746       709        5.960784             37        0  \n",
       "2747       539        5.725000             27        0  \n",
       "\n",
       "[2651 rows x 16 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.word_count(data,\"abstract\",\"word_cnt\")\n",
    "tf.char_count(data,\"abstract\",\"char_len\")\n",
    "tf.avg_word_length(data,\"abstract\",\"avg_wrd_length\")\n",
    "tf.stopwords_count(data,\"abstract\",\"stopwords_cnt\")\n",
    "tf.numerics_count(data,\"abstract\",\"num_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs', 'similarity_score',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "df = data[columns].copy()\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "#df = pd.DataFrame(x_scaled)\n",
    "df=pd.DataFrame(x_scaled, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yules_i_measure_abs</th>\n",
       "      <th>avg_word_len_abs</th>\n",
       "      <th>avg_sen_len_abs</th>\n",
       "      <th>freq_words_gt_sen_len_abs</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>char_len</th>\n",
       "      <th>avg_wrd_length</th>\n",
       "      <th>stopwords_cnt</th>\n",
       "      <th>num_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.265512</td>\n",
       "      <td>0.667844</td>\n",
       "      <td>0.829997</td>\n",
       "      <td>0.362416</td>\n",
       "      <td>0.915270</td>\n",
       "      <td>0.299703</td>\n",
       "      <td>0.365145</td>\n",
       "      <td>0.667844</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.159442</td>\n",
       "      <td>0.274419</td>\n",
       "      <td>0.401851</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.828280</td>\n",
       "      <td>0.412463</td>\n",
       "      <td>0.416782</td>\n",
       "      <td>0.274419</td>\n",
       "      <td>0.365672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.134984</td>\n",
       "      <td>0.696577</td>\n",
       "      <td>0.498123</td>\n",
       "      <td>0.395973</td>\n",
       "      <td>0.767135</td>\n",
       "      <td>0.400593</td>\n",
       "      <td>0.484555</td>\n",
       "      <td>0.696577</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.328817</td>\n",
       "      <td>0.427072</td>\n",
       "      <td>0.311583</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.875312</td>\n",
       "      <td>0.264095</td>\n",
       "      <td>0.287690</td>\n",
       "      <td>0.427072</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.161022</td>\n",
       "      <td>0.388069</td>\n",
       "      <td>0.216458</td>\n",
       "      <td>0.563758</td>\n",
       "      <td>0.859142</td>\n",
       "      <td>0.543027</td>\n",
       "      <td>0.566160</td>\n",
       "      <td>0.388069</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    yules_i_measure_abs  avg_word_len_abs  avg_sen_len_abs  \\\n",
       "14             0.265512          0.667844         0.829997   \n",
       "15             0.159442          0.274419         0.401851   \n",
       "18             0.134984          0.696577         0.498123   \n",
       "20             0.328817          0.427072         0.311583   \n",
       "25             0.161022          0.388069         0.216458   \n",
       "\n",
       "    freq_words_gt_sen_len_abs  similarity_score  word_cnt  char_len  \\\n",
       "14                   0.362416          0.915270  0.299703  0.365145   \n",
       "15                   0.409396          0.828280  0.412463  0.416782   \n",
       "18                   0.395973          0.767135  0.400593  0.484555   \n",
       "20                   0.234899          0.875312  0.264095  0.287690   \n",
       "25                   0.563758          0.859142  0.543027  0.566160   \n",
       "\n",
       "    avg_wrd_length  stopwords_cnt  num_len  \n",
       "14        0.667844       0.231343      0.0  \n",
       "15        0.274419       0.365672      0.0  \n",
       "18        0.696577       0.313433      0.0  \n",
       "20        0.427072       0.231343      0.0  \n",
       "25        0.388069       0.477612      0.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = data.sample(frac=0.8, random_state = 1)\n",
    "test_set = data.loc[~data.index.isin(training_set.index)]\n",
    "test_set.head()\n",
    "\n",
    "training_set = df.sample(frac=0.8, random_state = 1)\n",
    "test_set = df.loc[~df.index.isin(training_set.index)]\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['yules_i_measure_abs', 'avg_word_len_abs',\n",
    "       'avg_sen_len_abs', 'freq_words_gt_sen_len_abs']\n",
    "\n",
    "columns = ['yules_i_measure_abs', 'avg_word_len_abs', 'avg_sen_len_abs', 'freq_words_gt_sen_len_abs',\n",
    "          'word_cnt', 'char_len', 'avg_wrd_length', 'stopwords_cnt', 'num_len']\n",
    "\n",
    "\n",
    "# training_data = training_set.as_matrix(columns = data_columns)\n",
    "training_data = training_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(training_data)\n",
    "training_data[nan_locs] = 0\n",
    "\n",
    "training_target = training_set['similarity_score'].values\n",
    "nan_locs = np.isnan(training_target)\n",
    "training_target[nan_locs] = 0\n",
    "\n",
    "test_data = test_set[data_columns].to_numpy()# .as_matrix(columns = data_columns)\n",
    "nan_locs = np.isnan(test_data)\n",
    "test_data[nan_locs] = 0\n",
    "\n",
    "test_target = test_set['similarity_score'].values\n",
    "nan_locs = np.isnan(test_target)\n",
    "test_target[nan_locs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_array = training_data.copy()\n",
    "train_class_array = training_target.copy()\n",
    "test_data_array = test_data.copy()\n",
    "test_class_array = test_target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.09736815376029817\n",
      "MAE: 0.09681168668436005\n",
      "MSE: 0.01613762316897446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor()\n",
    "reg.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(reg.score(test_data, test_target)))\n",
    "pred = reg.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.12083272647376608\n",
      "MAE: 0.09569713013126403\n",
      "MSE: 0.015718113893018346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(svr.score(test_data, test_target)))\n",
    "pred = svr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.622802514678054\n",
      "MAE: 0.13460794179775187\n",
      "MSE: 0.02901313040154366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "kr = KernelRidge()\n",
    "kr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(kr.score(test_data, test_target)))\n",
    "pred = kr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.02427441258220786\n",
      "MAE: 0.1030722315686996\n",
      "MSE: 0.018312398970559727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(dt.score(test_data, test_target)))\n",
    "pred = dt.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.09653160340839584\n",
      "MAE: 0.09557588282393557\n",
      "MSE: 0.016152579360025232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 2000)\n",
    "rf.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(rf.score(test_data, test_target)))\n",
    "pred = rf.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.15165164255378272\n",
      "MAE: 0.09306300551973408\n",
      "MSE: 0.015167120643392318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(gb.score(test_data, test_target)))\n",
    "pred = gb.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.13146911554279306\n",
      "MAE: 0.09562408766059806\n",
      "MSE: 0.015527952157212524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pr = Pipeline([('poly', PolynomialFeatures(degree = 3)),\n",
    "              ('linear', LinearRegression(fit_intercept = False))])\n",
    "\n",
    "pr.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(pr.score(test_data, test_target)))\n",
    "pred = pr.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.008909479954041899\n",
      "MAE: 0.102671836181547\n",
      "MSE: 0.018037698389361555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lm = Lasso()\n",
    "lm.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(lm.score(test_data, test_target)))\n",
    "pred = lm.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.008909479954041899\n",
      "MAE: 0.102671836181547\n",
      "MSE: 0.018037698389361555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(en.score(test_data, test_target)))\n",
    "pred = en.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: -0.008909479954041899\n",
      "MAE: 0.102671836181547\n",
      "MSE: 0.018037698389361555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "ll = LassoLars()\n",
    "ll.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(ll.score(test_data, test_target)))\n",
    "pred = ll.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.08845326380264507\n",
      "MAE: 0.09684434561297495\n",
      "MSE: 0.01629700723605431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "br = BayesianRidge()\n",
    "br.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(br.score(test_data, test_target)))\n",
    "pred = br.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared: 0.042888133821536334\n",
      "MAE: 0.10017137972692909\n",
      "MSE: 0.017111639359154922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(training_data, training_target)\n",
    "\n",
    "print('R squared: ' + str(sgd.score(test_data, test_target)))\n",
    "pred = sgd.predict(test_data)\n",
    "print('MAE: ' + str(metrics.mean_absolute_error(test_target, pred)))\n",
    "print('MSE: ' + str(metrics.mean_squared_error(test_target, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{table}[ht]\n",
    "\\caption{Model Results} % title of Table\n",
    "\\centering % used for centering table\n",
    "\\begin{tabular}{c c c c} % centered columns (4 columns)\n",
    "\\hline\\hline %inserts double horizontal lines\n",
    "Case & Method\\#1 & Method\\#2 & Method\\#3 \\\\ [0.5ex] % inserts table\n",
    "%heading\n",
    "\\hline % inserts single horizontal line\n",
    "1 & 50 & 837 & 970 \\\\ % inserting body of the table\n",
    "2 & 47 & 877 & 230 \\\\\n",
    "3 & 31 & 25 & 415 \\\\\n",
    "4 & 35 & 144 & 2356 \\\\\n",
    "5 & 45 & 300 & 556 \\\\ [1ex] % [1ex] adds vertical space\n",
    "\\hline %inserts single line\n",
    "\\end{tabular}\n",
    "\\label{table:nonlin} % is used to refer this table in the text\n",
    "\\end{table}\n",
    "--------------------- e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Tuned Decision Tree Parameters: {'random_state': 1558, 'min_samples_split': 0.9, 'min_samples_leaf': 0.30000000000000004, 'max_features': 2, 'max_depth': 32, 'criterion': 'mse'}\n",
      "Best score is 0.06330274150685954\n",
      "Train MSE:   0.014\n",
      "Test MSE:   0.014\n",
      "Test MAE:   0.093\n",
      "R-squared  0.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist_dt = {\"max_depth\": [1, 32],\n",
    "              \"max_features\": list(range(1, 21)),\n",
    "              \"random_state\": list(range(1, 5000)),\n",
    "              \"min_samples_split\": np.linspace(0.1, 1, 10, endpoint=True),\n",
    "              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "              \"criterion\": [\"friedman_mse\", \"mse\"]}\n",
    "## Instantiate the RandomizedSearchCV object: tree_cv\n",
    "dt_cv = RandomizedSearchCV(DecisionTreeRegressor(), \\\n",
    "                           param_dist_dt, cv=10, n_jobs=-1, verbose=1)\n",
    "# Fit it to the data\n",
    "dt_cv.fit(train_data_array, train_class_array)\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(dt_cv.best_params_))\n",
    "print(\"Best score is {}\".format(dt_cv.best_score_))\n",
    "# train mse\n",
    "pred = dt_cv.predict(train_data_array)\n",
    "score = metrics.mean_squared_error(train_class_array, pred)\n",
    "print(\"Train MSE:   %0.3f\" % score)\n",
    "# test mse\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_squared_error(test_class_array, pred)\n",
    "print(\"Test MSE:   %0.3f\" % score)\n",
    "# test mae\n",
    "pred = dt_cv.predict(test_data_array)\n",
    "score = metrics.mean_absolute_error(test_class_array, pred)\n",
    "print(\"Test MAE:   %0.3f\" % score)\n",
    "# r-squared\n",
    "score = metrics.r2_score(test_class_array, pred)\n",
    "print(\"R-squared  %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
